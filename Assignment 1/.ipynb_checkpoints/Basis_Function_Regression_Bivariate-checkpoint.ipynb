{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "import matplotlib.font_manager\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d  \n",
    "import seaborn as sns  #A statistical plotting library\n",
    "from sklearn.cluster import KMeans\n",
    "from math import comb\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "#from kneed import KneeLocator\n",
    "from mayavi import mlab\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-13.475524</td>\n",
       "      <td>-15.229121</td>\n",
       "      <td>140554.303333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-15.230830</td>\n",
       "      <td>-12.205868</td>\n",
       "      <td>98205.871542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5.762633</td>\n",
       "      <td>-12.758111</td>\n",
       "      <td>54090.524314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>8.805995</td>\n",
       "      <td>6.387340</td>\n",
       "      <td>9342.283121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>15.734299</td>\n",
       "      <td>3.268716</td>\n",
       "      <td>61518.311357</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0         x1         x2              y\n",
       "0           0 -13.475524 -15.229121  140554.303333\n",
       "1           1 -15.230830 -12.205868   98205.871542\n",
       "2           2   5.762633 -12.758111   54090.524314\n",
       "3           3   8.805995   6.387340    9342.283121\n",
       "4           4  15.734299   3.268716   61518.311357"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DF = pd.read_csv('function1_2d.csv')\n",
    "DF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining a class to process and divide the dataframe in to the req. numpy form.\n",
    "class pro_split_df:\n",
    "    def __init__(self,df,batch_size):\n",
    "        self.df   = df                 #The original dataframe.\n",
    "        self.batch_size = batch_size   #Number of samples to be choosen from the dataframe.\n",
    "        \n",
    "        \n",
    "    #A function to convert dataframe in to numpy array.\n",
    "    #Training data set input vectors is stored in a matrix of dim Nxd. d -> no. of features, N -> No. of examples\n",
    "    def df_to_np(self,dat_fr):\n",
    "        In_df = dat_fr.loc[:,['x1','x2']]    #Choosing the columns belonging to the input feature vectors.\n",
    "        L_df  = dat_fr.loc[:,['y']]          #Choosing the columns belonging to the labels.\n",
    "        return In_df.to_numpy(),L_df.to_numpy()\n",
    "    \n",
    "    #A function for Random row selection of required batch size of a Pandas dataframe\n",
    "    def Rand_Choose(self):\n",
    "        return self.df.sample(n = self.batch_size, random_state=42)\n",
    "    \n",
    "    \n",
    "    def df_split(self,df):\n",
    "        #Splits the array in to 70,20,10.\n",
    "        return np.split(df,[int(.7*len(df)), int(.8*len(df))])\n",
    "    \n",
    "    #A function to split the batches in to training and data.   \n",
    "    def tr_val_test_split(self):\n",
    "        dat_fr = self.Rand_Choose()        #Randomly choosing batchsize number of samples from the org. dataframe.\n",
    "        tr_df,ts_df,val_df = self.df_split(dat_fr) #Splits the datframe in to train, val and test.\n",
    "        x_tr,y_tr = self.df_to_np(tr_df)      #conversion to numpy\n",
    "        x_val,y_val = self.df_to_np(val_df)\n",
    "        x_test,y_test = self.df_to_np(ts_df)\n",
    "        return x_tr,x_val,x_test,y_tr,y_val,y_test\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dividing the datset in to batches of 50,200 and 500.\n",
    "#N=50\n",
    "batch50 = pro_split_df(DF,50)  #An instace of class sep_data.\n",
    "x_tr50,x_val50,x_test50,y_tr50,y_val50,y_test50 = batch50.tr_val_test_split()\n",
    "\n",
    "#N=200\n",
    "batch200 = pro_split_df(DF,200)  #An instace of class sep_data.\n",
    "x_tr200,x_va200,x_test200,y_tr200,y_val200,y_test200 = batch200.tr_val_test_split()\n",
    "\n",
    "#N=500\n",
    "batch500 = pro_split_df(DF,500)  #An instace of class sep_data.\n",
    "x_tr500,x_val500,x_test500,y_tr500,y_val500,y_test500 = batch500.tr_val_test_split()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining a class that given our choice of basis function returns the optimal paramters.\n",
    "#Polynomial and Gaussian Basis for the regression task at hand is employed.\n",
    "\n",
    "class Do_Regression:\n",
    "    #This class will return the optimal paramters by using the basis function of our choice.\n",
    "    def __init__(self,X,y,Gb_Hp,Pb_Hp,basisname):\n",
    "        self.X = X  #Input vectors of training dataset as a numpy array Nxd.\n",
    "        self.y = y\n",
    "        self.Gb_Hp = Gb_Hp\n",
    "        self.D = None   #An hyperparameter which detrmines the spread of gaussian basis functions.\n",
    "        self.s = None  #An hyperparameter which detrmines the spread of gaussian basis functions.\n",
    "        self.Mean_Mat = None     #The mean vector array-(D-1)xd. Each row represents a mean vector.\n",
    "        self.Pb_Hp = Pb_Hp  #List of hyper\n",
    "        self.Erms = None\n",
    "        self.phi = None     #Attribute used to store the design matrix used for testing\n",
    "        self.pred =None     #Attribute to store the prediction values\n",
    "        self.basisname = basisname\n",
    "        self.hp_comb_gbf = None  #An attribute for the combination of hyperparameters.\n",
    "        \n",
    "           \n",
    "    #KMeans implementation for finding the mean of the various basis functions initially.\n",
    "    #Number of clusters eqauls the number of gaussian basis functions being used.\n",
    "    def K_Clustering(self):\n",
    "        #Dictionary of the arguments for scikit.KMeans\n",
    "        KMeans_args = {\n",
    "        \"init\" :\"random\",\n",
    "        \"n_init\" : 10,\n",
    "        \"max_iter\" : 300,\n",
    "        \"random_state\" : 0,\n",
    "        } \n",
    "        k = self.D - 1   #Number of clusters equals D-1.\n",
    "        kmeans =  KMeans(n_clusters = k , **KMeans_args)\n",
    "        kmeans.fit(self.X)\n",
    "        labels = kmeans.labels_\n",
    "        #The mean vector matrix is stored \n",
    "        self.Mean_Mat = kmeans.cluster_centers_\n",
    "        \n",
    "        \n",
    "    def Gaussian_hyperparamter_comb(self):\n",
    "        hp_list_gbf = list(self.Gb_Hp.values()) # a list of all possible combinations of hyperparameters\n",
    "        self.hp_comb_gbf = [(d,s) for d in hp_list_gbf[0] for s in hp_list_gbf[1]]\n",
    "        \n",
    " \n",
    "    def Gaussian_DesMat(self):\n",
    "        #Returns the output design matrix made of gbf of the input x.\n",
    "        phi_gauss = np.zeros((len(self.X), self.D))\n",
    "        #Finding the mean of clusters,\n",
    "        self.K_Clustering()   #Calling this function will update the attribute associated with Mean Matrix.\n",
    "        \n",
    "        ones_vector = np.ones((len(self.X))) #To be added to the first column of the matrix\n",
    "        phi_gauss[:,0] = ones_vector\n",
    "        #Components of the matrix.\n",
    "        c1 = np.reshape(np.einsum('ij -> i', self.X**2),(len(self.X),-1)) #A Nx1 array. N->No of examples in training\n",
    "        c2 = np.einsum('ij -> i', self.Mean_Mat**2)                       #A D-1 array.\n",
    "        c3 = np.einsum('ij,jk -> ik', self.X,(self.Mean_Mat.T))                           #A Nx(D-1) array.\n",
    "        expo = -(c1+c2-(2*c3))/2*self.s*self.s\n",
    "        phi_gauss[:,1:] = np.exp(expo)\n",
    "        return phi_gauss\n",
    "       \n",
    "    \n",
    "    def Poly_DesMat(self):\n",
    "        #Returns the output design matrix made of gbf of the input x and degree m.\n",
    "        d = len(x[0])\n",
    "        #m is the maximum degree of monomials being used for polynomial basis function.\n",
    "        poly = PolynomialFeatures(degree = self.m)  #Using the inbuilt function from sklearn.\n",
    "        phi_poly = poly.fit_transform(self.X)\n",
    "        \n",
    "        \n",
    "        #Plotting for better understanding\n",
    "#         phi_flat = phi_poly.flatten()\n",
    "#         xarr = np.arange(1,phi_flat.size+1,1)\n",
    "#         plt.scatter(xarr,phi_flat)\n",
    "#         plt.show()\n",
    "        return phi_poly\n",
    "    \n",
    "    def get_design_mat(self):\n",
    "        if self.basisname == 'Gaussian':\n",
    "            PHI = self.Gaussian_DesMat()   #Updates the design matrix with gaussian basis functions.\n",
    "        if self.basisname == 'Polynomial':\n",
    "            PHI = self.Poly_DesMat()        #Updates the design matrix with polynomial basis functions.\n",
    "#         if basisname == 'Linear':\n",
    "#             PHI = self.X\n",
    "        return PHI    \n",
    "    \n",
    "    def LinearRegressor(self,lam):  \n",
    "        #Takes as input the labels of the training data and the hyperparamter lambda\n",
    "        erms_tr ={}\n",
    "        W = {}\n",
    "        if self.basisname == 'Gaussian':\n",
    "            self.Gaussian_hyperparamter_comb()  #Updates the attribute for the combination of hyperparameters.\n",
    "            for (dim,sigma) in self.hp_comb_gbf:\n",
    "                self.D,self.s = dim,sigma        #Updates the attribute for hyperparameters of gbf\n",
    "                PHI = self.get_design_mat()\n",
    "                w = np.linalg.inv(PHI.T@PHI + lam*np.identity(len(PHI.T)))@(PHI.T@self.y)\n",
    "                #Error for training data\n",
    "                erms_tr[str(dim)+' '+ str(sigma)] = self.erms(w) \n",
    "                W[str(dim)+ ' ' + str(sigma)] = w\n",
    "        if self.basisname == 'Polynomial':\n",
    "            for degree in Pb_Hp:\n",
    "                self.m = degree\n",
    "                PHI = self.get_design_mat()\n",
    "                w = np.linalg.inv(PHI.T@PHI + lam*np.identity(len(PHI.T)))@(PHI.T@self.y)\n",
    "                W['degree'] = w\n",
    "        return W,erms_tr\n",
    "    \n",
    "    \n",
    "    def erms(self,w):\n",
    "        self.phi = self.get_design_mat()\n",
    "        self.pred = self.phi@w\n",
    "        error_arr = self.pred - self.y\n",
    "        return LA.norm(error_arr)/np.sqrt(len(self.y))\n",
    "    \n",
    "    \n",
    "    def test_set_error(self,W):  #W is the optimal parameters estimated from training\n",
    "        #Run this function only after training and on testing on validation and test dataset.\n",
    "        W_test = {}\n",
    "        self.Gaussian_hyperparamter_comb()\n",
    "        for (dim,sigma) in self.hp_comb_gbf:\n",
    "            self.D,self.s = dim,sigma \n",
    "            w = W[str(dim)+' '+ str(sigma)]\n",
    "            W_test[str(dim)+' '+ str(sigma)] = self.erms(w)\n",
    "        return W_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparamters for gaussian and polynomial.\n",
    "Gb_Hp = {'Dim': [6,9,25,35,50], 'sigma' : [0.01,0.1,1,10]}\n",
    "Pb_Hp = [2,3,6,9] \n",
    "do_reg = Do_Regression(x_tr500,y_tr500,Gb_Hp,Pb_Hp,'Gaussian')  #An instance for the regression class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optimal Parameters estimated using training data of batch 50(N=35) and lambda = 0 using gbf\n",
    "W_opt,erms_tr_data = do_reg.LinearRegressor(0) \n",
    "# mus= do_reg.Mean_Mat  #Centers after clustering.\n",
    "# #Plotting the cluster centers\n",
    "# plt.figure(figsize =(8,6))\n",
    "# plt.scatter(x_tr50[:,0],x_tr50[:,1],c='g',label='training data') #Input vector\n",
    "# plt.scatter(mus[:,0],mus[:,1],c='r', label = 'cluster centers')  #Cluster centers\n",
    "# plt.xlabel(r'$x1$')\n",
    "# plt.ylabel(r'$x2$')\n",
    "# plt.legend()\n",
    "# plt.title('Training Dataset with {} cluster centers'.format(Gb_Hp['Dim']-1))\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'6 0.01': 10872.227348499611,\n",
       " '6 0.1': 14606.186808332766,\n",
       " '6 1': 36645.92907689616,\n",
       " '6 10': 37083.62187569195,\n",
       " '9 0.01': 9825.05260525228,\n",
       " '9 0.1': 9287.13112013742,\n",
       " '9 1': 36304.04763645538,\n",
       " '9 10': 37059.10802018388,\n",
       " '25 0.01': 167816.46133443833,\n",
       " '25 0.1': 2123.588206526122,\n",
       " '25 1': 31930.87472312734,\n",
       " '25 10': 11277857.476872137,\n",
       " '35 0.01': 2507060.169039422,\n",
       " '35 0.1': 388.79706998297445,\n",
       " '35 1': 25997.154195725885,\n",
       " '35 10': 126190350.62860964,\n",
       " '50 0.01': 52204.30141142701,\n",
       " '50 0.1': 256.1429635181774,\n",
       " '50 1': 22140.15496762211,\n",
       " '50 10': 11281825.607481595}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Error for various sigma and dimensions of the training data.\n",
    "\n",
    "erms_tr_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 1)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Optimal Parameters after training\n",
    "W_opt['6 0.01'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'6 0.01': 2150559.3759162137,\n",
       " '6 0.1': 48372.71645765625,\n",
       " '6 1': 43890.581322829545,\n",
       " '6 10': 58153.547018613586,\n",
       " '9 0.01': 363543185.40609735,\n",
       " '9 0.1': 58187.318603779364,\n",
       " '9 1': 43061.33583203358,\n",
       " '9 10': 12545422256157.57,\n",
       " '25 0.01': 3051216504.327322,\n",
       " '25 0.1': 1817241.6859147162,\n",
       " '25 1': 54833.57768594001,\n",
       " '25 10': 4.261456604327142e+33,\n",
       " '35 0.01': 3760321791.278151,\n",
       " '35 0.1': 6818336.303514117,\n",
       " '35 1': 66733.39404213506,\n",
       " '35 10': 3.959192142244023e+35,\n",
       " '50 0.01': 3515235806.9307137,\n",
       " '50 0.1': 308448688.22739655,\n",
       " '50 1': 64646.70270686437,\n",
       " '50 10': 5.240631818694538e+47}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Validation dataset error estimation.\n",
    "Gb_Hp = {'Dim': [6,9,25,35,50], 'sigma' : [0.01,0.1,1,10]}\n",
    "val_reg = Do_Regression(x_val500,y_val500,Gb_Hp,Pb_Hp,'Gaussian')\n",
    "erms_val_data = val_reg.test_set_error(W_opt)\n",
    "# erms_val_df = pd.DataFrame(list(erms_val_data.items()),columns = ['Degree and Sigma','Erms'])\n",
    "erms_val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Predictions on the training dataset using in-built functions.\n",
    "# from sklearn.pipeline import make_pipeline\n",
    "# from sklearn.preprocessing import PolynomialFeatures\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "# from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #Polynomial basis Regression\n",
    "# poly_model = make_pipeline(PolynomialFeatures(6),LinearRegression())\n",
    "# pred=poly_model.fit(x_tr50,y_tr50).predict(x_tr50)\n",
    "# erms = erms_model(pred,y_tr50)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "As can be seen from the above plots, varying the values of sigma greatly varies the points. If many points are\n",
       "reduced then this might lead to an overfitting even in the case of 3 dimensions(3 parameters) problem as \n",
       "the number of parameters available will match the number of points.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex\n",
    "As can be seen from the above plots, varying the values of sigma greatly varies the points. If many points are\n",
    "reduced then this might lead to an overfitting even in the case of 3 dimensions(3 parameters) problem as \n",
    "the number of parameters available will match the number of points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

\section{Linear Model for Regression using Gaussian Basis Functions}
\subsection{\textcolor{teal}{Python Code}}

\begin{minted}[frame=lines, linenos, fontsize=\large]
{python}

# -*- coding: utf-8 -*-


import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

################################ Dataset 3 ###################################

def process_dataset_3(D, var, r):
    
    """
    D: No of gauss functions for fit
    var: variance for fit
    r: regularisation term
    """
    # Read txt file
    
    f = open('datasets/2_music.txt', 'r')
    
    length = 0
    for line in f:
        
        d = [float(i) for i in line.split(',')]
        
        if length == 0:
            data = np.array(d)
            data = data[np.newaxis,:]
        else:
            d = np.array(d)
            d = d[np.newaxis,:]
            data = np.append(data,d,axis=0)
        length = length+1
    
    f.close()
       
    data = pd.DataFrame(data)
    
    
    # shuffle dataset
    data = data.sample(frac=1)
    
    # get dependent and independent varaible
    data = np.array(data)
    
    
    X = data[:,0:-2]
    y = data[:,-2:]
    
    # get unique values of y
    y_unique = np.unique(y,axis=0)
    
    # length of data for fit
    train_len = int(np.shape(X)[0]*0.7)
    val_len = int(np.shape(X)[0]*0.2)
    test_len = int(np.shape(X)[0]) - val_len - train_len
    
    # test train split
    X_train = X[0:train_len]
    X_test = X[train_len:train_len+test_len]
    X_val = X[train_len+test_len:train_len+test_len+val_len]
    
    y_train = y[0:train_len]
    y_test = y[train_len:train_len+test_len]
    y_val = y[train_len+test_len:train_len+test_len+val_len]
    
    # K-means clustering
    loop = 1
    prev_zni = np.zeros((train_len,D-1))
    
    while(loop>0):
        zni = np.zeros((train_len,D-1))
        
        if loop == 1:
            # randomly choose k points
            random_index = np.random.randint(0,train_len,D-1)
        
            # Initialize MUi
            MUi = X_train[random_index,:]
        
        # Determine points belonging to clusters
        for j in range(0,train_len):
            i = np.argmin(np.linalg.norm(X_train[j,:]-MUi,axis=1),axis=0)
            zni[j,i] = 1
            
        # Determine number of datapoints in the clusters
        Ni = np.sum(zni,axis=0)
        
        # Update MUi
        for j in range(0,D-1):
            if Ni[j] == 0:
                continue
            
            z = zni[:,j]
            MUi[j,:] = (np.sum(X_train*z[:,np.newaxis],axis=0))/Ni[j]
            
        # compare previous and current Zni
        comp = prev_zni - zni
        
        prev_zni = zni
        loop = loop+1
        
        # if no change break the loop
        if np.max(comp) == 0 and np.min(comp) == 0:
            break
    
    # Assign Mui
    mu = MUi
    
    # gaussian function
    phii = lambda x,mu : np.e**(-(np.linalg.norm(x-mu))/var**2)
    
    # Formulate phi matrix for training data
    phin = np.zeros((train_len,D))
    
    for i in range(0,train_len):
        for j in range(0,D):
            if j==0:
                phin[i,j] = 1
                #phin[i,j] = phii(X_train[i,:],mu[j,:])
            else:
                phin[i,j] = phii(X_train[i,:],mu[j,:])
            
    # weights with quadratic regularisation
    W = np.linalg.inv(phin.T@phin + r*np.identity(D))@phin.T@y_train
    
    # weights with tikhanov regularisation
    # phin_bar = np.zeros((D,D))
    # for i in range(0,D):
    #     for j in range(0,D):
    #         phin_bar[i,j] = np.exp(np.linalg.norm(mu[i,:]-mu[j,:])/var**2)
    
    # W = np.linalg.inv(phin.T@phin + r*phin_bar)@phin.T@y_train
    
    ###########################################################################
    
    # Test on training dataset
    y_pred = np.zeros((train_len,2))
    
    # Formulate phi matrix for training data
    phin = np.zeros((train_len,D))
    
    for i in range(0,train_len):
        for j in range(0,D):
            if j==0:
                phin[i,j] = 1
                # phin[i,j] = phii(X_train[i,:],mu[j,:])
            else:
                phin[i,j] = phii(X_train[i,:],mu[j])
    
    for i in range(0,train_len):
        pred = (phin[i,:])@W
        y_pred[i,:] = pred
        
        # term1 = np.deg2rad((pred[0] - y_unique[:,0])/2)
        # term2 = np.deg2rad((pred[1] - y_unique[:,1])/2)
        # a = (np.sin(term1)**2) + # np.cos(np.deg2rad(pred[0]))*
        #      np.cos(np.deg2rad(y_unique[:,0]))*(np.sin(term2# )**2)
        # dist = 2*6373*np.arctan2(np.sqrt(a),np.sqrt(1-a))
        
        # index = np.argmin(dist)
        # y_pred[i,:] = y_unique[index,:]
    
    Erms_train1 = ((np.linalg.norm(y_pred[:,0]-y_train[:,0])))/np.sqrt(train_len)
    Erms_train2 = ((np.linalg.norm(y_pred[:,1]-y_train[:,1])))/np.sqrt(train_len)

    # # scatter plot
    # fig1 = plt.figure(figsize=(9,9))
    # plt.scatter(y_train[:,0],y_pred[:,0],marker='x')
    # plt.scatter(y_train[:,1],y_pred[:,1],marker='x')
    # plt.xlabel('target',fontsize=20)
    # plt.ylabel('predicted',fontsize=20)
    # plt.title("Gaussian Basis Function (For Dataset 3 of # best performing 
    #             model with training data) | degree = {} | # lambda = {}".format(D,r),
    #             fontsize = 20)
    # plt.legend(["y0","y1"])
    ###########################################################################
    
    # Test on validation dataset
    y_pred = np.zeros((val_len,2))
    
    # Formulate phi matrix for validation data
    phin = np.zeros((val_len,D))
    
    for i in range(0,val_len):
        for j in range(0,D):
            if j==0:
                phin[i,j] = 1
                # phin[i,j] = phii(X_val[i,:],mu[j,:])
            else:
                phin[i,j] = phii(X_val[i,:],mu[j])
    
    for i in range(0,val_len):
        pred = (phin[i,:])@W
        y_pred[i,:] = pred
    
    
    Erms_val1 = ((np.linalg.norm(y_pred[:,0]-y_val[:,0])))/np.sqrt(val_len)
    Erms_val2 = ((np.linalg.norm(y_pred[:,1]-y_val[:,1])))/np.sqrt(val_len)
    
    # scatter plot for validation data
    # fig1 = plt.figure(figsize=(9,9))
    # plt.scatter(y_val,y_pred,marker='x')
    # plt.xlabel('target',fontsize=20)
    # plt.ylabel('predicted',fontsize=20)
    # plt.title("Gaussian Basis Function (For Dataset 2 of best performing model 
    #            with validation data) | degree = {} | lambda # = {}".format(D,r),
    #             fontsize = 20)
    
    ###############################################################################
    
    # Test on test dataset
    y_pred = np.zeros((test_len,2))
    
    # Formulate phi matrix for test data
    phin = np.zeros((test_len,D))
    
    for i in range(0,test_len):
        for j in range(0,D):
            if j==0:
                phin[i,j] = 1
                #phin[i,j] = phii(X_test[i,:],mu[j,:])
            else:
                phin[i,j] = phii(X_test[i,:],mu[j])
    
    for i in range(0,test_len):
        pred = (phin[i,:])@W
        y_pred[i,:] = pred
        
    Erms_test1 = ((np.linalg.norm(y_pred[:,0]-y_test[:,0])))/np.sqrt(test_len)
    Erms_test2 = ((np.linalg.norm(y_pred[:,1]-y_test[:,1])))/np.sqrt(test_len)
    
    # scatter plot for test data
    fig1 = plt.figure(figsize=(9,9))
    plt.scatter(y_test[:,0],y_pred[:,0],marker='x')
    plt.scatter(y_test[:,1],y_pred[:,1],marker='x')
    plt.xlabel('target',fontsize=20)
    plt.ylabel('predicted',fontsize=20)
    plt.title("Gaussian Basis Function (For Dataset 3 of best performing model
               with test data) | degree = {} | lambda = {}".format(D,r),
                fontsize = 20)
    plt.legend(["y0","y1"])
    ###########################################################################
    return Erms_train1, Erms_train2, Erms_val1, Erms_val2, Erms_test1, Erms_test2
    
################################ Dataset 2 ###################################

def process_dataset_2(D, var, r):
    
    """
    D: No of gauss functions for fit
    var: variance for fit
    r: regularisation term
    """
    
    # Read csv file
    data = pd.read_csv('datasets/function1_2d.csv')
    
    # shuffle dataset
    # data = data.sample(frac=1)
    
    # get dependent and independent varaible
    X = data[['x1','x2']]
    y = data['y']
    
    # convert into numpy
    X = np.array(X)
    y = np.array(y)
    
    # length of data for fit
    train_len = int(np.shape(X)[0]*0.7)
    test_len = int(np.shape(X)[0]*0.2)
    val_len = int(np.shape(X)[0]*0.1)
    
    # test train split
    X_train = X[0:train_len]
    X_test = X[train_len:train_len+test_len]
    X_val = X[train_len+test_len:train_len+test_len+val_len]
    
    y_train = y[0:train_len]
    y_test = y[train_len:train_len+test_len]
    y_val = y[train_len+test_len: train_len+test_len+val_len]
    
    # K-means clustering
    loop = 1
    prev_zni = np.zeros((train_len,D-1))
    
    while(loop>0):
        zni = np.zeros((train_len,D-1))
        
        if loop == 1:
            # randomly choose k points
            random_index = np.random.randint(0,train_len,D-1)
        
            # Initialize MUi
            MUi = X_train[random_index,:]
        
        # Determine points belonging to clusters
        for j in range(0,train_len):
            i = np.argmin(np.linalg.norm(X_train[j,:]-MUi,axis=1),axis=0)
            zni[j,i] = 1
            
        # Determine number of datapoints in the clusters
        Ni = np.sum(zni,axis=0)
        
        # Update MUi
        for j in range(0,D-1):
            if Ni[j] == 0:
                continue
            
            z = zni[:,j]
            MUi[j,:] = (np.sum(X_train*z[:,np.newaxis],axis=0))/Ni[j]
            
        # compare previous and current Zni
        comp = prev_zni - zni
        
        prev_zni = zni
        loop = loop+1
        
        # if no change break the loop
        if np.max(comp) == 0 and np.min(comp) == 0:
            break
    
    # Assign Mui
    mu = MUi
    
    # gaussian function
    phii = lambda x,mu : np.e**(-(np.linalg.norm(x-mu))/var**2)
    
    # Formulate phi matrix for training data
    phin = np.zeros((train_len,D))
    
    for i in range(0,train_len):
        for j in range(0,D):
            if j==0:
                phin[i,j] = 1
            else:
                phin[i,j] = phii(X_train[i,:],mu[j-1,:])
            
    
    # weights with quadratic regularisation
    W = np.linalg.inv(phin.T@phin + r*np.identity(D))@phin.T@y_train
    
    ###########################################################################
    
    # Test on training dataset
    y_pred = np.zeros((train_len,1))
    
    # Formulate phi matrix for training data
    phin = np.zeros((train_len,D))
    
    for i in range(0,train_len):
        for j in range(0,D):
            if j==0:
                phin[i,j] = 1
            else:
                phin[i,j] = phii(X_train[i,:],mu[j-1])
    
    for i in range(0,train_len):
        y_pred[i,:] = (phin[i,:])@W
        
    Erms_train = ((np.linalg.norm(y_pred-y_train[:,np.newaxis])))/
                   np.sqrt(train_len)
    
    
    # # scatter plot for training data
    # fig1 = plt.figure(figsize=(9,9))
    # plt.scatter(y_train,y_pred,marker='x')
    # plt.xlabel('target',fontsize=20)
    # plt.ylabel('predicted',fontsize=20)
    # plt.title("Gaussian Basis Function (For Dataset 2 of best performing model
    #             with training data) | degree = {} | lambda = # {}".format(D,r),
    #              fontsize = 20)
    
    
    ###########################################################################
    
    # Test on validation dataset
    y_pred = np.zeros((val_len,1))
    
    # Formulate phi matrix for validation data
    phin = np.zeros((val_len,D))
    
    for i in range(0,val_len):
        for j in range(0,D):
            if j==0:
                phin[i,j] = 1
            else:
                phin[i,j] = phii(X_val[i,:],mu[j-1])
    
    for i in range(0,val_len):
        y_pred[i,:] = (phin[i,:])@W
        
    Erms_val = ((np.linalg.norm(y_pred-y_val[:,np.newaxis])))/np.sqrt(val_len)
    
    # scatter plot for validation data
    # fig1 = plt.figure(figsize=(9,9))
    # plt.scatter(y_val,y_pred,marker='x')
    # plt.xlabel('target',fontsize=20)
    # plt.ylabel('predicted',fontsize=20)
    # plt.title("Gaussian Basis Function (For Dataset 2 of best performing model 
    #             with validation data) | degree = {} | lambda #= {}".format(D,r),
    #              fontsize = 20)
    
    ###########################################################################
    
    # Test on test dataset
    y_pred = np.zeros((test_len,1))
    
    # Formulate phi matrix for test data
    phin = np.zeros((test_len,D))
    
    for i in range(0,test_len):
        for j in range(0,D):
            if j==0:
                phin[i,j] = 1
            else:
                phin[i,j] = phii(X_test[i,:],mu[j-1])
    
    for i in range(0,test_len):
        y_pred[i,:] = (phin[i,:])@W
        
    Erms_test = ((np.linalg.norm(y_pred-y_test[:,np.newaxis])))/np.sqrt(test_len)
    
    # # scatter plot for validation data
    # fig1 = plt.figure(figsize=(9,9))
    # plt.scatter(y_test,y_pred,marker='x')
    # plt.xlabel('target',fontsize=20)
    # plt.ylabel('predicted',fontsize=20)
    # plt.title("Gaussian Basis Function (For Dataset 2 of best performing 
    #             model with test data) | degree = {} | lambda # = {}".format(D,r),
    #              fontsize = 20)
    
    ###########################################################################
    return Erms_train, Erms_val, Erms_test
    

############################ Main Code ########################################

# Call function to experiment dataset 2/3
D = [2,3,6,9,20,33,40,100]
var = [1,3,5,10,50,100]
reg = [0,10e-5,10e-4,10e-3,10e-2,10e-1,10,10e2,10e3,10e4,10e5]

Erms = []
# Experiment with variance
for v in var:
    erms = process_dataset_3(100, v, 0)
     Erms.append(erms)
    
# Plot table
fig = plt.figure(figsize=(9,9))
row_labels=["var = 1", "var = 3", "var = 5", "var = 10", "var = 50", "var = 100"]
column_labels=["Erms_train", "Erms_val", "Erms_test"]
column_labels=["Erms_train_y0","Erms_train_y1", "Erms_val0","Erms_val1",
                 "Erms_test0", "Erms_test1"]
plt.axis('tight')
plt.axis('off')
plt.table(cellText=Erms,rowLabels=row_labels,colLabels=column_labels, loc="center")

Erms = []
# Experiment with Dimensions
for d in D:
    erms = process_dataset_3(d, 3, 0)
    Erms.append(erms)

# Plot table
fig = plt.figure(figsize=(9,9))
row_labels=["D = 2", "D = 3", "D = 6", "D = 9", "D = 20", "D = 30",
              "D = 40", "D = 100"]
column_labels=["Erms_train", "Erms_val", "Erms_test"]
column_labels=["Erms_train_y0","Erms_train_y1", "Erms_val0","Erms_val1", 
                 "Erms_test0", "Erms_test1"]
plt.axis('tight')
plt.axis('off')
plt.table(cellText=Erms,rowLabels=row_labels,colLabels=column_labels,
            loc="center")

Erms = []  
# Experiment with regularisation
for r in reg:
    erms = process_dataset_3(100, 3, r)
    Erms.append(erms)
    
# Plot table
fig = plt.figure(figsize=(9,9))
row_labels=["lambda = 0", "lambda = 10e-5", "lambda = 10e-4", "lambda = 10e-3", 
              "lambda = 10e-2", "lambda = 10e-1", "lambda = 10", "lambda = 10e2", 
              "lambda = 10e3", "lambda = 10e4", "lambda = 10e5"]
column_labels=["Erms_train", "Erms_val", "Erms_test"]
column_labels=["Erms_train_y0","Erms_train_y1", "Erms_val0","Erms_val1", 
                 "Erms_test0", "Erms_test1"]
plt.axis('tight')
plt.axis('off')
plt.table(cellText=Erms,rowLabels=row_labels,colLabels=column_labels,
            loc="center")
\end{minted}

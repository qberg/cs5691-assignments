\section{Static Pattern Classification on Real World Dataset 2A}


\subsection{\textcolor{teal}{Python Code}}

\begin{minted}[frame=lines, linenos, fontsize=\large]
{python}


import numpy as np
import pandas as pd
from sklearn import preprocessing
from sklearn.cluster import KMeans
from matplotlib import pyplot as plt
import math

#scaling factor
scalingfac = 1

#extracting and parsing data
#preprocessing : quantile transformation
f = pd.read_csv('Dataset_2A/coast/train.csv')
f1 = f.to_numpy()
data1 = f1[:,1:]
data1 = data1.astype('float')
data1 = np.apply_along_axis(lambda x:np.append(x,np.array([1,0,0,0,0])),1,data1)
X1 = data1[:,:-5]
X1 = scalingfac*X1
X1 = (preprocessing.QuantileTransformer(random_state=0)).fit_transform(X1)
f = pd.read_csv('Dataset_2A/forest/train.csv')
f1 = f.to_numpy()
data2 = f1[:,1:]
data2 = data2.astype('float')
data2 = np.apply_along_axis(lambda x:np.append(x,np.array([0,1,0,0,0])),1,data2)
X2 = data2[:,:-5]
X2 = scalingfac*X2
X2 = (preprocessing.QuantileTransformer(random_state=0)).fit_transform(X2)
f = pd.read_csv('Dataset_2A/mountain/train.csv')
f1 = f.to_numpy()
data3 = f1[:,1:]
data3 = data3.astype('float')
data3 = np.apply_along_axis(lambda x:np.append(x,np.array([0,0,1,0,0])),1,data3)
X3 = data3[:,:-5]
X3 = scalingfac*X3
X3 = (preprocessing.QuantileTransformer(random_state=0)).fit_transform(X3)
f = pd.read_csv('Dataset_2A/opencountry/train.csv')
f1 = f.to_numpy()
data4 = f1[:,1:]
data4 = data4.astype('float')
data4 = np.apply_along_axis(lambda x:np.append(x,np.array([0,0,0,1,0])),1,data4)
X4 = data4[:,:-5]
X4 = scalingfac*X4
X4 = (preprocessing.QuantileTransformer(random_state=0)).fit_transform(X4)
f = pd.read_csv('Dataset_2A/street/train.csv')
f1 = f.to_numpy()
data5 = f1[:,1:]
data5 = data5.astype('float')
data5 = np.apply_along_axis(lambda x:np.append(x,np.array([0,0,0,0,1])),1,data5)
X5 = data5[:,:-5]
X5 = scalingfac*X5
X5 = (preprocessing.QuantileTransformer(random_state=0)).fit_transform(X5)

data = np.concatenate((data1,data2,data3,data4,data5),axis=0)

#prior probabilities
py1 = len(data1)/len(data)
py2 = len(data2)/len(data)
py3 = len(data3)/len(data)
py4 = len(data4)/len(data)
py5 = len(data5)/len(data)
ppy = np.array([py1,py2,py3,py4,py5])  

#hyperparameters
threshold = 1e-12
nclasses = 5
#option for diagonal(opt=0) or full(opt=1) covariance matrix
opt = int(input('covariance'))
#number of clusters in each class
q = list(map(int,input('clusters').split(' ')))

# initialisation of parameters
fweights = {}
fmeans = {}
fvariances = {}

#GMM for each class
def GMMperclass(X,q,threshold,opt):  #X = data q = cluster #hyperparameter
    dim = X.shape[1]
    N = len(X)
    
    #kmeans clustering
    kmeans = KMeans(init='random',n_clusters=q,n_init=20,max_iter=800)
    kmeans.fit(X)
    response = np.zeros((N,q))
    labl = kmeans.labels_
    
    #gamma matrix
    for i in range(len(labl)):
        response[i,labl[i]] = 1
        
    #gaussian for individual datapoint
    def gauss(x,u,v):
        dim = len(x)
        num = ((np.reshape((x-u),(1,dim)))@((np.linalg.inv(v))@
                                  (np.reshape((x-u),(dim,1)))))
        num = -num/2
        den = np.sqrt(((2*np.pi)**dim)*(np.linalg.det(v)))
        return ((np.exp(num))/den)
     
    #gaussian for whole datapoints
    def gaussmat(X,q,means,variances):
        N,dim = X.shape
        #Inverse of the covariance matrix
        sigma_inv = np.linalg.inv(variances)     
        mean_shift = np.reshape(X, (N, 1, dim) ) - np.reshape(means, (1,q, dim) )
        #Normalisation term of the Gaussian dist
        norm = np.sqrt(((2*np.pi)**dim)*np.linalg.det(variances))  
        #Exponential term of the Gaussian
        expo = np.exp(-0.5*(np.einsum("nkj,nkj->nk", np.einsum("nki,kij->nkj",
                      mean_shift, sigma_inv),mean_shift)))  
        return expo/norm
    
    #updating gamma=response update
    def responseupdate(weights,X,q,means,variances):
        N,dim = X.shape 
        No = gaussmat(X,q,means,variances)
        Num =  No * weights
        Den = np.reshape(np.sum(Num, axis=1), (N, 1) )
        return Num/Den

    #estimation of log likelihoods
    def llhd(weights,X,q,means,variances):
        return np.sum(np.log(gaussmat(X,q,means,variances) @ weights))

    ##initialisation
    weights = np.einsum("ij -> j",response) / N
    means = (((response).T) @ X)/ np.reshape((weights*N), (q, 1))
    Nki = N*weights
    mean_shifti = np.reshape(X, (N, 1, dim) ) - np.reshape(means, (1, q, dim) )
    sigmai = np.einsum("nki,nkj->kij", np.einsum("nk,nki->nki", response,
                 mean_shifti), mean_shifti) / np.reshape(Nki, (q, 1, 1))

    #full covariance matrix
    if opt == 1: 
        variances = sigmai
    #diagonal covariance matrix
    if opt == 0: 
        I = np.identity(dim) 
        variances = np.einsum("kij,ij -> kij",sigmai,I)
    
    #initial log likelihood
    NLL = llhd(weights,X,q,means,variances)
    nweights = weights
    nmeans = means
    nvariances = variances
    OLL = NLL+10
    ite = 0
    
    #EM maximisation
    while abs(NLL-OLL)>=threshold:
        OLL = NLL
        #gamma calculation
        nresponse = responseupdate(nweights,X,q,nmeans,nvariances)
        #updation
        nweights = np.einsum("ij -> j",nresponse)/N
        nmeans = (((nresponse).T) @X) / np.reshape((nweights*N), (q, 1))
        Nk = N*nweights
        mean_shift = np.reshape(X, (N, 1, dim) ) - np.reshape(nmeans, (1, q, dim) )
        sigma = np.einsum("nki,nkj->kij", np.einsum("nk,nki->nki", 
                     nresponse, mean_shift), mean_shift) / np.reshape(Nk, (q, 1, 1))
            
        #full covariance matrix
        if opt == 1: 
            nvariances = sigma
            
        #full diagonal matrix
        if opt == 0: 
            I = np.identity(dim) 
            nvariances = np.einsum("kij,ij -> kij",sigma,I)
        

        NLL = llhd(nweights,X,q,nmeans,nvariances)
        ite+=1
    print("iterations=%f"%ite)
    
    #final parameters
    fweights = nweights
    fmeans = nmeans
    fvariances = nvariances
    return fweights,fmeans,fvariances

##training

fweights[0],fmeans[0],fvariances[0] = GMMperclass(X1,q[0],threshold,opt)
fweights[1],fmeans[1],fvariances[1] = GMMperclass(X2,q[1],threshold,opt)
fweights[2],fmeans[2],fvariances[2] = GMMperclass(X3,q[2],threshold,opt)
fweights[3],fmeans[3],fvariances[3] = GMMperclass(X4,q[3],threshold,opt)
fweights[4],fmeans[4],fvariances[4] = GMMperclass(X5,q[4],threshold,opt)

#modeloutput

def bayesclf(x,ppy,nclasses,q,fweights,fmeans,fvariances):
    def gauss(x,u,v):
        dim = len(x)
        num = ((np.reshape((x-u),(1,dim)))@((np.linalg.inv(v))@
                                   (np.reshape((x-u),(dim,1)))))
        num = -num/2
        den = np.sqrt(((2*np.pi)**dim)*(np.linalg.det(v)))
        return ((np.exp(num))/den)
    def gaussmat(X,q,means,variances):
        N,dim = X.shape
        sigma_inv = np.linalg.inv(variances)     
        mean_shift = np.reshape(X, (N, 1, dim) ) - np.reshape(means, (1,q, dim) )
        norm = np.sqrt(((2*np.pi)**dim)*np.linalg.det(variances))  
        expo = np.exp(-0.5*(np.einsum("nkj,nkj->nk", np.einsum("nki,kij->nkj", 
                                          mean_shift, sigma_inv),mean_shift)))  
        return expo/norm
    pxy = np.zeros((nclasses,1))
    pyx = np.zeros((nclasses,1))
    res = np.zeros((nclasses,1))
    for i in range(nclasses):
        pxy[i] = np.sum((gaussmat(np.reshape(x,(1,len(x))),q[i],fmeans[i],
                                            fvariances[i]))*(fweights[i]))
    for i in range(nclasses):
        pyx[i] = pxy[i]*ppy[i]
    pyx = pyx/np.sum(pyx)
    res[np.argmax(pyx)] = 1
    return np.transpose(res)

#train labels
ytr = data[:,-5:]
#predicted train labels
ytrp = np.apply_along_axis(lambda x:bayesclf(x,ppy,nclasses,q,fweights,fmeans,
                        fvariances),1,np.concatenate((X1,X2,X3,X4,X5),axis=0))
c = 0
for i in range(len(ytr)):
    if np.linalg.norm((ytrp[i,:]-ytr[i,:])) < 1:
        c +=1
print('training accuracy = %f'%(c/len(ytr)))
    
###########test

#extracting test data
#preprocessing : quantile transformation
f = pd.read_csv('Dataset_2A/coast/dev.csv')
f1 = f.to_numpy()
datat1 = f1[:,1:]
datat1 = datat1.astype('float')
datat1 = np.apply_along_axis(lambda x:np.append(x,np.array([1,0,0,0,0])),1,datat1)
Xt1 = datat1[:,:-5]
Xt1 = scalingfac*Xt1
Xt1 = ((preprocessing.QuantileTransformer(random_state=0)).fit(Xt1)).transform(Xt1)
f = pd.read_csv('Dataset_2A/forest/dev.csv')
f1 = f.to_numpy()
datat2 = f1[:,1:]
datat2 = datat2.astype('float')
datat2 = np.apply_along_axis(lambda x:np.append(x,np.array([0,1,0,0,0])),1,datat2)
Xt2 = datat2[:,:-5]
Xt2 = scalingfac*Xt2
Xt2 = ((preprocessing.QuantileTransformer(random_state=0)).fit(Xt2)).transform(Xt2)
f = pd.read_csv('Dataset_2A/mountain/dev.csv')
f1 = f.to_numpy()
datat3 = f1[:,1:]
datat3 = datat3.astype('float')
datat3 = np.apply_along_axis(lambda x:np.append(x,np.array([0,0,1,0,0])),1,datat3)
Xt3 = datat3[:,:-5]
Xt3 = scalingfac*Xt3
Xt3 = ((preprocessing.QuantileTransformer(random_state=0)).fit(Xt3)).transform(Xt3)
f = pd.read_csv('Dataset_2A/opencountry/dev.csv')
f1 = f.to_numpy()
datat4 = f1[:,1:]
datat4 = datat4.astype('float')
datat4 = np.apply_along_axis(lambda x:np.append(x,np.array([0,0,0,1,0])),1,datat4)
Xt4 = datat4[:,:-5]
Xt4 = scalingfac*Xt4
Xt4 = ((preprocessing.QuantileTransformer(random_state=0)).fit(Xt4)).transform(Xt4)
f = pd.read_csv('Dataset_2A/street/dev.csv')
f1 = f.to_numpy()
datat5 = f1[:,1:]
datat5 = datat5.astype('float')
datat5 = np.apply_along_axis(lambda x:np.append(x,np.array([0,0,0,0,1])),1,datat5)
Xt5 = datat5[:,:-5]
Xt5 = scalingfac*Xt5
Xt5 = ((preprocessing.QuantileTransformer(random_state=0)).fit(Xt5)).transform(Xt5)

datat = np.concatenate((datat1,datat2,datat3,datat4,datat5),axis=0)
Xt = np.concatenate((Xt1,Xt2,Xt3,Xt4,Xt5),axis=0)
datat = np.concatenate((Xt,datat[:,-5:]),axis=1)

#test labels
yte = datat[:,-5:]

#predicted test labels
ytep = np.apply_along_axis(lambda x:bayesclf(x,ppy,nclasses,q,fweights,fmeans,
                           fvariances),1,datat[:,:-5])

cte = 0

for i in range(len(ytep)):
    if np.linalg.norm((ytep[i,:]-yte[i,:])) < 1:
        cte +=1

print('test accuracy = %f'%(cte/len(ytep)))
    


#plotting confusion matrix
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

def clf_label(res):
    if np.argmax(res) == 0:
        return 'coast'
    if np.argmax(res) == 1:
        return 'forest'
    if np.argmax(res) == 2:
        return 'mountain'
    if np.argmax(res) == 3:
        return 'opencountry'
    if np.argmax(res) == 4:
        return 'street'

train_confusion = confusion_matrix([clf_label(ytr[i,:]) for i in range(len(ytr))],
                   [clf_label(ytrp[i,:]) for i in range(len(ytrp))],labels = 
                   ['coast','forest','mountain','opencountry','street'])

test_confusion = confusion_matrix([clf_label(yte[i,:]) for i in range(len(yte))],
                       [clf_label(ytep[i,:]) for i in range(len(ytep))],labels =
                       ['coast','forest','mountain','opencountry','street'])

train_tab = ConfusionMatrixDisplay(train_confusion,display_labels =
['coast','forest','mountain','opencountry','street'] )
plt.figure(1)
train_tab.plot()
plt.title('Training Data')
test_tab = ConfusionMatrixDisplay(test_confusion,display_labels =
['coast','forest','mountain','opencountry','street'] )
plt.figure(2)
test_tab.plot()
plt.title('Test Data')






















\end{minted}
\section{Pattern classification on non-linearly separable data}
\subsection{K nearest Neighbours Method and Bayes classifier with KNN for density estimation}
\subsubsection{\textcolor{teal}{Python Code}}

\begin{minted}[frame=lines, linenos, fontsize=\large]
{python}

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix
from mlxtend.plotting import plot_confusion_matrix
from sklearn.metrics import accuracy_score
from sklearn.cluster import KMeans
from scipy.stats import multivariate_normal
#############################################################################
# train data
f = open('datasets/Dataset_1b/train.csv', 'r')
    
length = 0
for line in f:
        
    d = [float(i) for i in line.split(',')]
        
    if length == 0:
        data = np.array(d)
        data = data[np.newaxis,:]
    else:
        d = np.array(d)
        d = d[np.newaxis,:]
        data = np.append(data,d,axis=0)
    length = length+1
    
f.close()
       
train_data = pd.DataFrame(data)

# shuffle dataset
# train_data = train_data.sample(frac=1)

# get train data
train_data = np.array(train_data)

# test data
f = open('datasets/Dataset_1b/dev.csv', 'r')
    
length = 0
for line in f:
        
    d = [float(i) for i in line.split(',')]
        
    if length == 0:
        data = np.array(d)
        data = data[np.newaxis,:]
    else:
        d = np.array(d)
        d = d[np.newaxis,:]
        data = np.append(data,d,axis=0)
    length = length+1
    
f.close()
       
test_data = pd.DataFrame(data)

# shuffle dataset
test_data = test_data.sample(frac=1)

# get test data
test_data = np.array(test_data)
##############################################################################

# Split training data
# length of data for fit
train_len = int(np.shape(train_data)[0])
val_len = int(np.shape(test_data)[0]*0.5)
test_len = int(np.shape(test_data)[0]*0.5)

X_train = train_data[:,0:2]
X_val = test_data[0:val_len,0:2]
X_test = test_data[val_len:val_len+test_len,0:2]

y_train = train_data[:,2]
y_val = test_data[0:val_len,2]
y_test = test_data[val_len:val_len+test_len,2]

KNN classifier

# Build KNN classifier
y_pred = []
def KNN_classifier(K, x):
    """
    Parameters
    ----------
    K : value of nearest neighbours
    x : feature vector

    Returns
    -------
    None.
    """
    # find distance between feature vector and training data
    dist = np.linalg.norm(x-X_train,axis=1)
    
    # get the top index for the minimum distance
    min_dist_index = np.argsort(dist)   
    topk = min_dist_index[0:K]
    
    # get class of corresponding class
    K_class = y_train[topk]
    
    # get count of each class
    unique_class, counts = np.unique(K_class, return_counts=1)
    
    # get the index of max counts
    max_count = np.argmax(counts)
    
    # choose that as the class
    y_pred.append(unique_class[max_count])

K = 15
# test on training data
for i in range(0, train_len):
    KNN_classifier(K, X_train[i])

y_pred = np.array(y_pred)    

train_accuracy = accuracy_score(y_train,y_pred)*100
print("training accuracy: " , train_accuracy)

# plot confusion matrix for training data
c_matrix  = confusion_matrix(y_train, y_pred)
fig, ax = plot_confusion_matrix(conf_mat=c_matrix,figsize=(7,7),cmap=plt.cm.RdYlBu_r)
ax.set(title = "Confusion Matrix")
plt.show()

y_pred = []
# test on validation data
for i in range(0, val_len):
    KNN_classifier(K, X_val[i])
        
y_pred = np.array(y_pred)    

val_accuracy = accuracy_score(y_val,y_pred)*100
print("validation accuracy: " , val_accuracy)

y_pred = []
# test on test data
for i in range(0, test_len):
    KNN_classifier(K, X_test[i])
        
y_pred = np.array(y_pred)    

test_accuracy = accuracy_score(y_test,y_pred)*100
print("test accuracy: " , test_accuracy)

# plot confusion matrix for test data
c_matrix  = confusion_matrix(y_test, y_pred)
fig, ax = plot_confusion_matrix(conf_mat=c_matrix,figsize=(7,7),cmap=plt.cm.RdYlBu_r)
ax.set(title = "Confusion Matrix")
plt.show()

# Decision Region Plots #####################################################
# define bounds of the domain
min1, max1 = X_train[:, 0].min()-1, X_train[:, 0].max()+1
min2, max2 = X_train[:, 1].min()-1, X_train[:, 1].max()+1

# define the x and y scale
x1_grid = np.arange(min1, max1, 0.1)
x2_grid = np.arange(min2, max2, 0.1)

x1_grid, x2_grid = np.meshgrid(x1_grid, x2_grid)

c1, c2 = x1_grid.flatten(), x1_grid.flatten()
c1, c2 = x1_grid.reshape((len(c1), 1)), x2_grid.reshape((len(c2), 1))

x = np.hstack((c1,c2))

y_pred = []
for i in range(0, np.shape(x)[0]):
    KNN_classifier(K, x[i,:])

y_pred = np.array(y_pred)

x3_grid = y_pred.reshape(x1_grid.shape)

fig = plt.figure()
ax = fig.add_subplot(111)
ax.contourf(x1_grid, x2_grid, x3_grid, cmap='Paired')
ax.scatter(X_train[:,0],X_train[:,1],marker='x')
ax.scatter(X_test[:,0],X_test[:,1],marker='x')
ax.set_xlabel('x1',fontsize=20)
ax.set_ylabel('x2',fontsize=20)
ax.set_title('KNN model with K = 15', fontsize=20)

############################################################
# Bayes classifier with KNN for density estimation

# Build KNN for density estimation
y_pred = []
def KNN(K, x, X):
    """
    Parameters
    ----------
    K : value of nearest neighbours
    x : feature vector
    X : training data related to particular class
    Returns
    -------
    None.
    """
    # find distance between feature vector and training data
    dist = np.linalg.norm(x-X,axis=1)
    
    # get the top k index for the minimum distance
    min_dist_index = np.argsort(dist)   
    topk = min_dist_index[0:K]
    
    # radius is distance of kth neearest neighbour
    R = dist[topk[-1]]
    
    return R

# split into different classes
unique_class,counts = np.unique(y_train,return_counts=1)
total_class = len(unique_class)

class_data = []

for i in range(0,total_class):
    class_data.append(X_train[y_train==i])

K=20
# bayes classifier -> this is just the min value of R for all the classes
# upon simplification of the actual bayes theorem 
# test on train data
y_pred = np.zeros((train_len,))
for i in range(train_len):
    decision = []
    for j in range(0,total_class):
        decision.append(KNN(K,X_train[i],class_data[j]))
    y_pred[i] = np.argmin(decision)

train_accuracy = accuracy_score(y_train,y_pred)*100
print("training accuracy: " , train_accuracy)

# plot confusion matrix for training data
c_matrix  = confusion_matrix(y_train, y_pred)
fig, ax = plot_confusion_matrix(conf_mat=c_matrix,figsize=(7,7),cmap=plt.cm.RdYlBu_r)
ax.set(title = "Confusion Matrix")
plt.show()

# test on validation data
y_pred = np.zeros((val_len,))
for i in range(val_len):
    decision = []
    for j in range(0,total_class):
        decision.append(KNN(K,X_val[i],class_data[j]))
    y_pred[i] = np.argmin(decision)

val_accuracy = accuracy_score(y_val,y_pred)*100
print("validation accuracy: " , val_accuracy)

# test on test data
y_pred = np.zeros((test_len,))
for i in range(test_len):
    decision = []
    for j in range(0,total_class):
        decision.append(KNN(K,X_test[i],class_data[j]))
    y_pred[i] = np.argmin(decision)

y_test = np.array(y_test)
test_accuracy = accuracy_score(y_test,y_pred)*100
print("test accuracy: " , test_accuracy)

# plot confusion matrix for test data
c_matrix  = confusion_matrix(y_test, y_pred)
fig, ax = plot_confusion_matrix(conf_mat=c_matrix,figsize=(7,7),cmap=plt.cm.RdYlBu_r)
ax.set(title = "Confusion Matrix")
plt.show()
 
    
# Decision Region Plots #####################################################
# define bounds of the domain
min1, max1 = X_train[:, 0].min()-1, X_train[:, 0].max()+1
min2, max2 = X_train[:, 1].min()-1, X_train[:, 1].max()+1

# define the x and y scale
x1_grid = np.arange(min1, max1, 0.1)
x2_grid = np.arange(min2, max2, 0.1)

x1_grid, x2_grid = np.meshgrid(x1_grid, x2_grid)

c1, c2 = x1_grid.flatten(), x1_grid.flatten()
c1, c2 = x1_grid.reshape((len(c1), 1)), x2_grid.reshape((len(c2), 1))

x = np.hstack((c1,c2))

y_pred = np.zeros((np.shape(x)[0],))
for i in range(np.shape(x)[0]):
    decision = []
    for j in range(0,total_class):
        decision.append(KNN(K,x[i],class_data[j]))
    y_pred[i] = np.argmin(decision)

x3_grid = y_pred.reshape(x1_grid.shape)

fig = plt.figure()
ax = fig.add_subplot(111)
ax.contourf(x1_grid, x2_grid, x3_grid, cmap='Paired')
ax.scatter(X_train[:,0],X_train[:,1],marker='x')
ax.set_xlabel('x1',fontsize=20)
ax.set_ylabel('x2',fontsize=20)
ax.set_title('Bayes Classifier with KNN density estimation', fontsize=20)

\end{minted}

\subsection{Bayes Classifier with GMM}
\subsubsection{\textcolor{teal}{Python Code}}

\begin{minted}[frame=lines, linenos, fontsize=\large]
{python}

#!/usr/bin/env python
# coding: utf-8

# In[1]:


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns  #A statistical plotting library
from sklearn.cluster import KMeans
from kneed import KneeLocator  #A function that helps in optimization of 
                               #number of clusters from an error curve
from scipy.stats import multivariate_normal as mvn
from mlxtend.plotting import plot_confusion_matrix
from sklearn.metrics import confusion_matrix


# In[2]:


header_names = ['x1', 'x2' , 'Class']
D = pd.read_csv('datasets/Dataset_1b/train.csv', header = None, names = header_names)
D.head()


# In[3]:


L_df = D.loc[:,['x1','x2']]
Unlab_Data = L_df.to_numpy()

lab_df = D.loc[:,'Class']
labels = lab_df.to_numpy()

#Training Dataset for Class 0
L0 = (D['Class'] == 0.0)
L0_df = D.loc[L0 , ['x1', 'x2'] ]  
Class0 = L0_df.to_numpy()

#Training Dataset for Class 1
L1 = (D['Class'] == 1.0)
L1_df = D.loc[L1 , ['x1', 'x2']]
Class1 = L1_df.to_numpy()

#Training Dataset for Class 2
L2 = (D['Class'] == 2.0)
L2_df = D.loc[L2, ['x1', 'x2']]
Class2 = L2_df.to_numpy()
labels.shape


# In[4]:


#KMeans implementation for initialization and optimization of the number of
#clusters.
#Number of clusters for each class eqauls the number of gaussian componenets 
#to be fitted for that class.
def K_Clustering(Class,M):
    #Dictionary of the arguments for scikit.KMeans
    KMeans_args = {
        "init" :"random",
        "n_init" : 10,
        "max_iter" : 300,
        "random_state" : 0,
        }
    #Estimation of the optimum number of clusters using elbow method
    std_error = []
    for cluster in range(1,11):
        kmeans = KMeans(n_clusters = cluster , **KMeans_args)
        kmeans.fit(Class)
        std_error.append(kmeans.inertia_)
    if M==0:
        #detecting the elbow point of the curve of 's_err vs K' using kneed, which
        #gives the optimum number of clusters
        curve = KneeLocator(range(1,11), std_error, curve="convex",
        direction = "decreasing")
        K_opt = curve.elbow
    else:
        #Using Manually entered value for K_opt
        K_opt = M
    #clustering the class in to K_opt clusters 
    kmeans =  KMeans(n_clusters = K_opt , **KMeans_args)
    kmeans.fit(Class)
    labels = kmeans.labels_
    centers = kmeans.cluster_centers_
    return K_opt,labels,centers


# In[5]:
        
#initialization of the parameters using K-Clusters

def Parameters_old(Class,M):
    #Will return a mean(mu)-(K,d) array;
    N,d = np.shape(Class)
    K,lab,mu = K_Clustering(Class,M)
    #gamma contains initial responsibilty values for an example w.r.t
    #each clusters as columns
    gamma = np.array([ [0]*K for i in range(N)])
    for example in range(N):
        for cluster in range(K):
            if lab[example] == cluster:
                gamma[example][cluster]= 1
    return K,mu,gamma


# In[6]:



#Defining the Gaussian Mixture Model as a class

class Gaussian_Mixture_Model:
    #Class - Examples of the class to which the Gaussian Componenets 
    #need to be fitted
    #Class - N x d matrix, where N is the number of examples and
    #d is the number of features for each example
    #K - Number of Gaussian Components that needs to be fitted
    
    def __init__(self,Class,K,MU,GAMMA,f): 
        self.Class = Class
        self.K = K   #Attribute for Number of clusters
        self.GAMMA = GAMMA          #Attribute for NxK array of posterior 
                                    #prob. / responsibity term.
        self.MU = MU                #Attribute for the mean values. An Kxd array.
        self.SIGMA = None           #Attribute for (K,d,d) array of covariances
        self.W = None               #Attribute for prior probabilty, 
                                    #an array of length K
        #self.max_iter = max_iter   #Attribute for the number of iterations
        self.N = len(self.Class)    #Attribute for number of examples available
        self.d = len(self.Class[0]) #Attribute for the number of features
                                    #in each example
        self.f = f                  #Attribute that acts as switch between
                                    #diagonal and full covariance matrices
        self.mean_shift = np.reshape(self.Class, (self.N, 1, self.d) ) -
                                     np.reshape(self.MU, (1, self.K, self.d) )
    
    def Prior_Probability(self):
        #A function to estimate the (K,) array of prior prob.
        self.W = np.einsum("ij -> j",self.GAMMA) / self.N  
        
    def Mean(self):
      # A function to calculate mean
      self.MU =  ((self.GAMMA).T) @ (self.Class) / np.reshape((self.W*self.N),
                  (self.K, 1)) 
      
    def Covariance_Matrix_Array(self):
        # A function to calculate covariances of the features of the examples
        
        Nk = np.einsum("ij -> j",self.GAMMA)
        self.mean_shift = np.reshape(self.Class, (self.N, 1, self.d) ) -
                                    np.reshape(self.MU, (1, self.K, self.d) )
                                    
        sigma = np.einsum("nki,nkj->kij", np.einsum("nk,nki->nki", self.GAMMA,
                          self.mean_shift), self.mean_shift) / np.reshape(Nk, 
                                                              (self.K, 1, 1))
            
        if self.f==1: #Case where we use full diagonal covariance matrix
            self.SIGMA = sigma
        
        if self.f==0: #Case where we use a diagonal covariance matrix
            I = np.identity(self.d,dtype=int) #An identity matrix of the size 
            #equal to number of feature
            
            self.SIGMA = np.einsum("kij,ij -> kij",sigma,I)
            
             
    def Gaussian_Prob(self):
        #This function accounts for our assumption that the conditional 
        #distribution of an example is a Gaussian.
        
        self.Covariance_Matrix_Array()           #SIGMA gets updated to the 
                                                  #full covariance matrix
        SIGMA_inv = np.linalg.inv(self.SIGMA)     #Inverse of the covariance matrix
        
        #Normalisation term of the Gaussian dist.
        norm = np.sqrt(((2*np.pi)**self.d)*np.linalg.det(self.SIGMA))
        
        
        #Exponential term of the Gaussian
        expo = np.exp(-0.5*(np.einsum("nkj,nkj->nk", np.einsum("nki,kij->nkj", 
                      self.mean_shift, SIGMA_inv),self.mean_shift)))  
        
        #Prob_mat is an (NxK)-array that contains Gaussian Prob. of the 
        #various examples to belong to respective clusters 
        Prob_mat =  expo / norm
        return Prob_mat
    
    def Expectation_Step(self):
        #In this step we update the values of the responsibilty term
        
        N = self.Gaussian_Prob()
        #Prior probability array
        self.W =  np.einsum("ij -> j",self.GAMMA) / self.N  
        Num =  N * self.W
        Den = np.reshape(np.sum(Num, axis=1), (self.N, 1) )
        self.GAMMA = Num/Den
      
    def Maximization_Step(self): 
        #In this step we updtae the various parameters
        
        #Updation of GAMMA
        self.Expectation_Step()
        
        #Updation of W
        self.Prior_Probability()
        
        #Updation of Mean MU
        self.Mean()
        
        #Updation of Covariance Matrix SIGMA
        self.Covariance_Matrix_Array()
      
    
    def Log_Likelihood(self):
      
      llhd = np.sum(np.log(self.Gaussian_Prob() @ self.W))
    
      return llhd
      
    
    def fit(self,max_iter,threshold):
        
        log_likelihoods = []  #Attribute for 1D array that contains Log_Likelihood 
                              # values. 
                              #Size depends on the number iterations required 
                              # to converge
        
        
        for i in range(max_iter):
            self.Expectation_Step()   #Updates Gamma
            self.Maximization_Step()  #Updates all the other parameters
            log_likelihoods.append(self.Log_Likelihood())
            #An if conditional for the requirement of convergence
            if (i!=0) & ((log_likelihoods[i] - log_likelihoods[i-1]) < threshold):
                    break
                    
        print("Number of iterations to convegre:" ,i)
        
    def plot(self,ax,x1_grid,x2_grid):  
        # #Plotting log_likelihood vs iterations, comment out if not needed
        # sns.set_style("darkgrid")         #setting the plot style
        # fig = plt.figure(figsize=(10,10))
        # ax0 = fig.add_subplot(111) 
        # ax0.set_title('Log-Likelihood')
        # ax0.plot(range(i+1),log_likelihoods)  
        
        #Plot of the fitted Gaussians for each class
        XY = np.array([x1_grid.flatten(),x2_grid.flatten()]).T 
        
        for mu,sigma in zip(self.MU,self.SIGMA):
            multi_normal = mvn(mean=mu,cov=sigma)
            contours = ax.contour(x1_grid, x2_grid, multi_normal.pdf(XY).reshape(
            len(x1_grid),len(x1_grid)),cmap='hsv',levels=4,extend='min')
            ax.clabel(contours, inline=1, fontsize=5)
    
    def Class_Prob(self,Y):
            #A function that returns Prob. 
            # for a unlabelled vector Y to belong to a class
            #Pred_Prob = []
            Multi_Gauss = []
            for mu,sigma in zip(self.MU,self.SIGMA):
                Multi_Gauss.append(mvn(mean=mu,cov=sigma).pdf(Y)) 
                #An array of Multi-Variate Gaussian Prob of various clusters                                                                         
            Wt_Gauss = np.einsum("i,i->i",self.W,Multi_Gauss) 
            #An array of weighted probabilities
            Pred_Prob =np.sum(Wt_Gauss)  
            return Pred_Prob
            
# In[7]:


#Fitting gaussian mixtures for Class0 
K,MU,GAMMA = Parameters_old(Class0,10)  
#0 as the second argument chooses by default K_opt estimated using elbow method. 
#If not pass the number of clusters needed

gmm0 = Gaussian_Mixture_Model(Class0,K,MU,GAMMA,1) 
# 0 as the last argument -> diagonal covariance matrix. 
# 1-> full covariance matix.   
gmm0.fit(max_iter=100,threshold = 1e-10)


# In[8]:


#Fitting gaussian mixtures for Class1 
K,MU,GAMMA = Parameters_old(Class1,10)
gmm1 = Gaussian_Mixture_Model(Class1,K,MU,GAMMA,1)
gmm1.fit(max_iter=100,threshold = 1e-10)


# In[9]:


#Fitting gaussian mixtures for Class2 
K,MU,GAMMA = Parameters_old(Class2,10)
gmm2 = Gaussian_Mixture_Model(Class2,K,MU,GAMMA,1)
gmm2.fit(max_iter=100,threshold = 1e-10)


# In[10]:
l1 = len(Class0)
l2 = len(Class1)
l3 = len(Class2)
total = l1+l2+l3

prior = []
prior.append(l1/total)
prior.append(l2/total)
prior.append(l3/total)

# We have fitted gaussians to each class and now we would like to make prediction 
# for unlabelled points
def Class_Prediction(Y):
    # gmm0,gmm1,gmm2 are the instances of class 0, class 1 and class 2 respectively
    n = len(Y) #number of unlabelled examples
    prediction = []
    for example in range(n):
        Prob=[]
        Prob = [gmm0.Class_Prob(Y[example,:])*prior[0], gmm1.Class_Prob(Y[example,:])*prior[1],gmm2.Class_Prob(Y[example,:])*prior[2] ]
        prediction.append(np.argmax(Prob))
    # print("Labels for the given dataset:", prediction)
    return prediction
    
# In[11]:

header_names = ['x1', 'x2' , 'Class']
D = pd.read_csv('datasets/Dataset_1b/dev.csv', header = None, names = header_names)
D.head()

# In[12]:

L_df = D.loc[:,['x1','x2']]
X_dev = L_df.to_numpy()

lab_df = D.loc[:,'Class']
y_dev = lab_df.to_numpy()

# In[12]:

# Divide into test and validation set
from sklearn.model_selection import train_test_split
X_val,X_test,y_val,y_test = train_test_split(X_dev,y_dev, test_size=0.5)

# In[14]:
from sklearn.metrics import accuracy_score
y_train = labels
X_train = np.concatenate((Class0,Class1,Class2))

predictions = Class_Prediction(X_train)

train_accuracy = accuracy_score(y_train,predictions)*100
print("train accuracy: " , train_accuracy)

# plot confusion matrix for training data
c_matrix  = confusion_matrix(y_train, predictions)
fig, ax = plot_confusion_matrix(conf_mat=c_matrix,figsize=(7,7),cmap=plt.cm.RdYlBu_r)
ax.set(title = "Confusion Matrix")
plt.show()

# In[15]:
predictions = Class_Prediction(X_val)

val_accuracy = accuracy_score(y_val,predictions)*100
print("val accuracy: " , val_accuracy)

# In[16]:
predictions = Class_Prediction(X_test)

test_accuracy = accuracy_score(y_test,predictions)*100
print("test accuracy: " , test_accuracy)

# plot confusion matrix for test data
c_matrix  = confusion_matrix(y_test, predictions)
fig, ax = plot_confusion_matrix(conf_mat=c_matrix,figsize=(7,7),cmap=plt.cm.RdYlBu_r)
ax.set(title = "Confusion Matrix")
plt.show()

# In[16]:

# Plot decision surface with training data and GMM superimposed
# define bounds of the domain
min1, max1 = X_train[:, 0].min()-1, X_train[:, 0].max()+1
min2, max2 = X_train[:, 1].min()-1, X_train[:, 1].max()+1

# define the x and y scale
x1_grid = np.arange(min1, max1, 0.1)
x2_grid = np.arange(min2, max2, 0.1)

x1_grid, x2_grid = np.meshgrid(x1_grid, x2_grid)

c1, c2 = x1_grid.flatten(), x1_grid.flatten()
c1, c2 = x1_grid.reshape((len(c1), 1)), x2_grid.reshape((len(c2), 1))

x = np.hstack((c1,c2))

# predict
predictions = np.array(Class_Prediction(x))

x3_grid = predictions.reshape(x1_grid.shape)

# plot decision surfaxe
fig = plt.figure(figsize=[13,13])
ax = fig.add_subplot(111)
ax.contourf(x1_grid, x2_grid, x3_grid, cmap='Pastel1')
ax.scatter(X_train[:,0],X_train[:,1],marker='x')
gmm0.plot(ax,x1_grid,x2_grid) # call to plot gaussian functions of class 0
gmm1.plot(ax,x1_grid,x2_grid) # call to plot gaussian functions of class 1
gmm2.plot(ax,x1_grid,x2_grid) # call to plot gaussian functions of class 2
ax.set_xlabel('x1',fontsize=20)
ax.set_ylabel('x2',fontsize=20)
ax.set_title('Bayes Classifier with GMM', fontsize=20)


\end{minted}
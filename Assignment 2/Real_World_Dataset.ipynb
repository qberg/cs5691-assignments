{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "modified-registration",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns  #A statistical plotting library\n",
    "from sklearn.cluster import KMeans\n",
    "from kneed import KneeLocator  #A function that helps in optimization of number of clusters from an error curve\n",
    "from scipy.stats import multivariate_normal as mvn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "intense-crystal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(251, 36, 23)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_file_as_arr(Dir):\n",
    "    #Returns a 3D array DAT\n",
    "    dat = [] #the list that needs to be converted to 3D array DAT\n",
    "    for image in os.listdir(Dir):\n",
    "        f = open(Dir +'/'+image)\n",
    "        single_image_dat = []\n",
    "        for line in f:\n",
    "            single_image_dat.append([float(x) for x in line.strip().split(' ')])\n",
    "        dat.append(single_image_dat)\n",
    "    DAT = np.array(dat)\n",
    "    l = DAT.shape[0]\n",
    "    m = DAT.shape[1]\n",
    "    DAT_Tr = DAT.reshape((l*m),-1)\n",
    "    return DAT,DAT_Tr,l     #len(DAT) is needed for prior prob. calculation as the number of \n",
    "                            #examples are different for each class\n",
    "\n",
    "\n",
    "#Class coast training images        \n",
    "Dir_ctr = 'Data/Dataset_2B/coast/train'             \n",
    "coast,coast_tr,n_c = load_file_as_arr(Dir_ctr) \n",
    "\n",
    "\n",
    "\n",
    "#Class forest training images\n",
    "Dir_ftr = 'Data/Dataset_2B/forest/train' \n",
    "forest,forest_tr,n_f = load_file_as_arr(Dir_ftr)\n",
    "\n",
    "\n",
    "#Class mountain training images\n",
    "Dir_mtr = 'Data/Dataset_2B/mountain/train'  \n",
    "mountain,mountain_tr,n_m = load_file_as_arr(Dir_mtr)\n",
    "\n",
    "\n",
    "#Class opencountry training images\n",
    "Dir_otr = 'Data/Dataset_2B/opencountry/train'  \n",
    "opencountry,opencountry_tr, n_o = load_file_as_arr(Dir_otr)\n",
    "\n",
    "#Class street training images\n",
    "Dir_str = 'Data/Dataset_2B/street/train'  \n",
    "street,street_tr, n_s = load_file_as_arr(Dir_str)\n",
    "\n",
    "Dir_coast_dev = 'Data/Dataset_2B/coast/dev' \n",
    "\n",
    "\n",
    "#Prior prob. for respective classes as an 1x5 array\n",
    "initial_weight = np.array([n_c, n_f, n_m, n_o, n_s])\n",
    "prior_prob = initial_weight/np.sum(initial_weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "elegant-religious",
   "metadata": {},
   "outputs": [],
   "source": [
    "#KMeans implementation for initialization and optimization of the number of clusters\n",
    "#Number of clusters for each class eqauls the number of gaussian componenets to be fitted for that class\n",
    "def K_Clustering(Class,M):\n",
    "    #Dictionary of the arguments for scikit.KMeans\n",
    "    KMeans_args = {\n",
    "        \"init\" :\"random\",\n",
    "        \"n_init\" : 10,\n",
    "        \"max_iter\" : 300,\n",
    "        \"random_state\" : 0,\n",
    "        }\n",
    "    #Estimation of the optimum number of clusters using elbow method\n",
    "    std_error = []\n",
    "    for cluster in range(1,11):\n",
    "        kmeans = KMeans(n_clusters = cluster , **KMeans_args)\n",
    "        kmeans.fit(Class)\n",
    "        std_error.append(kmeans.inertia_)\n",
    "    if M==0:\n",
    "        #detecting the elbow point of the curve of 's_err vs K' using kneed, which gives the optimum number of clusters\n",
    "        curve = KneeLocator(range(1,11), std_error, curve=\"convex\", direction = \"decreasing\")\n",
    "        K_opt = curve.elbow\n",
    "    else:\n",
    "        #Using Manually entered value for K_opt\n",
    "        K_opt = M\n",
    "    #clustering the class in to K_opt clusters \n",
    "    kmeans =  KMeans(n_clusters = K_opt , **KMeans_args)\n",
    "    kmeans.fit(Class)\n",
    "    labels = kmeans.labels_\n",
    "    centers = kmeans.cluster_centers_\n",
    "    return K_opt,labels,centers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "necessary-wrestling",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialization of the parameters using K-Clusters\n",
    "\n",
    "def Parameters_old(Class,M):\n",
    "    #Will return a mean(mu)-(K,d) array;\n",
    "    N,d = np.shape(Class)\n",
    "    K,lab,mu = K_Clustering(Class,M)\n",
    "    #gamma contains initial responsibilty values for an example w.r.t each clusters as columns\n",
    "    gamma = np.array([ [0]*K for i in range(N)])\n",
    "    for example in range(N):\n",
    "        for cluster in range(K):\n",
    "            if lab[example] == cluster:\n",
    "                gamma[example][cluster]= 1\n",
    "    return K,mu,gamma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "turned-computer",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the Gaussian Mixture Model as a class\n",
    "\n",
    "class Gaussian_Mixture_Model:\n",
    "    #Class - Examples of the class to which the Gaussian Componenets need to be fitted\n",
    "    #Class - N x d matrix, where N is the number of examples and d is the number of features for each example\n",
    "    #K - Number of Gaussian Components that needs to be fitted\n",
    "    \n",
    "    def __init__(self,Class,K,MU,GAMMA,f): \n",
    "        self.Class = Class\n",
    "        self.K = K   #Attribute for Number of clusters\n",
    "        self.GAMMA = GAMMA          #Attribute for NxK array of posterior responsibity term \n",
    "        self.MU = MU                #Attribute for the mean values. An Kxd array.\n",
    "        self.SIGMA = None           #Attribute for (K,d,d) array of covariances\n",
    "        self.W = None               #Attribute for prior probabilty, an array of length K\n",
    "        #self.max_iter = max_iter   #Attribute for the number of iterations\n",
    "        self.N = len(self.Class)    #Attribute for number of examples available\n",
    "        self.d = len(self.Class[0]) #Attribute for the number of features in each example\n",
    "        self.f = f                  #Attribute that acts as switch between diagonal and full covariance matrix\n",
    "        self.mean_shift = np.reshape(self.Class, (self.N, 1, self.d) ) - np.reshape(self.MU, (1, self.K, self.d) )\n",
    "    \n",
    "    def Prior_Probability(self):\n",
    "        #A function to estimate the (K,) array of prior prob.\n",
    "        self.W = np.einsum(\"ij -> j\",self.GAMMA) / self.N  \n",
    "        \n",
    "    def Mean(self):\n",
    "      # A function to calculate mean\n",
    "      self.MU =  ((self.GAMMA).T) @ (self.Class) / np.reshape((self.W*self.N), (self.K, 1)) \n",
    "  \n",
    "    def Covariance_Matrix_Array(self):\n",
    "        # A function to calculate covariances of the features of the examples\n",
    "        \n",
    "        Nk = np.einsum(\"ij -> j\",self.GAMMA)\n",
    "        self.mean_shift = np.reshape(self.Class, (self.N, 1, self.d) ) - np.reshape(self.MU, (1, self.K, self.d) )\n",
    "        sigma = np.einsum(\"nki,nkj->kij\", np.einsum(\"nk,nki->nki\", self.GAMMA, self.mean_shift), \n",
    "                                   self.mean_shift) / np.reshape(Nk, (self.K, 1, 1))\n",
    "            \n",
    "        if self.f==1: #Case where we use full diagonal covariance matrix\n",
    "            self.SIGMA = sigma\n",
    "        \n",
    "        if self.f==0: #Case where we use a diagonal covariance matrix\n",
    "            I = np.identity(self.d,dtype=int) #An identity matrix of the size equal to number of feature\n",
    "            self.SIGMA = np.einsum(\"kij,ij -> kij\",sigma,I)\n",
    "  \n",
    "\n",
    "    def Gaussian_Prob(self):\n",
    "        #This function accounts for our assumption that the conditional distribution of an example is a Gaussian.\n",
    "        \n",
    "        self.Covariance_Matrix_Array()           #SIGMA gets updated to the full covariance matrix\n",
    "        SIGMA_inv = np.linalg.inv(self.SIGMA)     #Inverse of the covariance matrix\n",
    "        \n",
    "        norm = np.sqrt(((2*np.pi)**self.d)*np.linalg.det(self.SIGMA))  #Normalisation term of the Gaussian dist.\n",
    "        #Exponential term of the Gaussian\n",
    "        expo = np.exp(-0.5*(np.einsum(\"nkj,nkj->nk\", np.einsum(\"nki,kij->nkj\", self.mean_shift, SIGMA_inv),\n",
    "                                     self.mean_shift)))  \n",
    "        \n",
    "        #Prob_mat is an (NxK)-array that contains Gaussian Prob. of the various examples to belong to \n",
    "        #respective clusters \n",
    "        Prob_mat =  expo / norm\n",
    "        return Prob_mat\n",
    "    \n",
    "    \n",
    "    def Expectation_Step(self):\n",
    "        #In this step we update the values of the responsibilty term\n",
    "        \n",
    "        N = self.Gaussian_Prob()\n",
    "        self.W =  np.einsum(\"ij -> j\",self.GAMMA) / self.N  #Prior probability array\n",
    "        Num =  N * self.W\n",
    "        Den = np.reshape(np.sum(Num, axis=1), (self.N, 1) )\n",
    "        self.GAMMA = Num/Den\n",
    "    \n",
    "      \n",
    "    def Maximization_Step(self): \n",
    "        #In this step we updtae the various parameters\n",
    "        \n",
    "        #Updation of GAMMA\n",
    "        self.Expectation_Step()\n",
    "        \n",
    "        #Updation of W\n",
    "        self.Prior_Probability()\n",
    "        \n",
    "        #Updation of Mean MU\n",
    "        self.Mean()\n",
    "        \n",
    "        #Updation of Covariance Matrix SIGMA\n",
    "        self.Covariance_Matrix_Array()\n",
    "      \n",
    "   \n",
    "    def Log_Likelihood(self):\n",
    "      \n",
    "      llhd = np.sum(np.log(self.Gaussian_Prob() @ self.W))\n",
    "    \n",
    "      return llhd\n",
    "      \n",
    "    \n",
    "    def fit(self,max_iter,threshold):\n",
    "        \n",
    "        log_likelihoods = []  #Attribute for 1D array that contains Log_Likelihood values. Size depends on the number iterations\n",
    "                                #required to converge\n",
    "        \n",
    "        \n",
    "        for i in range(max_iter):\n",
    "            self.Expectation_Step()   #Updates Gamma\n",
    "            self.Maximization_Step()  #Updates all the other parameters\n",
    "            log_likelihoods.append(self.Log_Likelihood())\n",
    "            #An if conditional for the requirement of convergence\n",
    "            if (i!=0) & ((log_likelihoods[i] - log_likelihoods[i-1]) < threshold):\n",
    "                    break\n",
    "                    \n",
    "        print(\"Number of iterations to converge:\" ,i)\n",
    "        \n",
    "        \n",
    "#         #Plotting log_likelihood vs iterations, comment out if not needed\n",
    "#         sns.set_style(\"darkgrid\")         #setting the plot style\n",
    "#         fig = plt.figure(figsize=(10,10))\n",
    "#         ax0 = fig.add_subplot(111) \n",
    "#         ax0.set_title('Log-Likelihood')\n",
    "#         ax0.plot(range(i+1),log_likelihoods)  \n",
    "        \n",
    "#         #Plot of the fitted Gaussians for each class\n",
    "#         x,y = np.meshgrid(np.sort(self.Class[:,0]),np.sort(self.Class[:,1])) # the meshgrid for the plot\n",
    "#         XY = np.array([x.flatten(),y.flatten()]).T \n",
    "#         sns.set_style(\"darkgrid\")         #setting the plot style\n",
    "#         fig1 = plt.figure(figsize=(10,10))\n",
    "#         ax1 = fig1.add_subplot(111)\n",
    "#         ax1.set_title('Fitted Gaussians')\n",
    "#         ax1.scatter(self.Class[:,0],self.Class[:,1],c= 'r')\n",
    "#         for mu,sigma in zip(self.MU,self.SIGMA):\n",
    "#             multi_normal = mvn(mean=mu,cov=sigma)\n",
    "#             ax1.contour(np.sort(self.Class[:,0]),np.sort(self.Class[:,1]),multi_normal.pdf(XY).reshape(len(self.Class),len(self.Class)),colors='black',alpha=0.3)\n",
    "            \n",
    "    \n",
    "    def Class_Prob(self,Y):\n",
    "            #call this for prediction of 1b\n",
    "            #A function that returns Prob. for a unlabelled vector Y to belong to a class\n",
    "            #Pred_Prob = []\n",
    "            Multi_Gauss = []\n",
    "            for mu,sigma in zip(self.MU,self.SIGMA):\n",
    "                Multi_Gauss.append(mvn(mean=mu,cov=sigma).pdf(Y)) #An array of Multi-Variate Gaussian Prob of various clusters                                                                         \n",
    "            Wt_Gauss = np.einsum(\"i,i->i\",self.W,Multi_Gauss) #An array of weighted probabilities\n",
    "            Pred_Prob =np.sum(Wt_Gauss)  \n",
    "            return Pred_Prob\n",
    "        \n",
    "    def Class_Prob_set_features(self,L):\n",
    "        #call this for prediction of 2b\n",
    "        #A function that returns Prob. for a set of feature vectors. For example, the set of all local feature \n",
    "        #vectors of an image. L will be an 2D array-(nxd).\n",
    "        n = L.shape[0]   #Number of local feature row vectors\n",
    "        d = L.shape[1]   #dimension  of feature space\n",
    "        Prob_nfeature_list = []\n",
    "        for n_feature in range(n):\n",
    "            Prob_nfeature_list.append(self.Class_Prob(L[n_feature,:d]))\n",
    "        \n",
    "        Prob_nfeature_arr = np.array(Prob_nfeature_list)\n",
    "        Pred_Prob = np.product(Prob_nfeature_arr)\n",
    "        return Pred_Prob\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "devoted-robert",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations to convegre: 1\n",
      "Number of iterations to convegre: 75\n",
      "Number of iterations to convegre: 49\n",
      "Number of iterations to convegre: 101\n",
      "Number of iterations to convegre: 68\n",
      "Number of iterations to convegre: 62\n",
      "Number of iterations to convegre: 61\n",
      "Number of iterations to convegre: 83\n"
     ]
    }
   ],
   "source": [
    "#Fitting GMM for class coast\n",
    "for clusters in range(1,9):\n",
    "    K,MU,GAMMA = Parameters_old(coast_tr,clusters)     #0 as the second argument chooses by default K_opt estimated using elbow method. \n",
    "                                                       #If not pass the number of clusters needed\n",
    "    gmm_coast = Gaussian_Mixture_Model(coast_tr,K,MU,GAMMA,1) #0 as the last argument -> diagonal covariance matrix\n",
    "    gmm_coast.fit(max_iter=1000,threshold = 1e-3)\n",
    "    gmm_coast\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "official-dodge",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 23)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gmm_coast.MU.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "streaming-strategy",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

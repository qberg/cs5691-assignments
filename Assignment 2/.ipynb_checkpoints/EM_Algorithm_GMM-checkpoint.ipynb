{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "vanilla-romance",
   "metadata": {},
   "source": [
    "# Dataset_1b GMM Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "polished-orientation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numba in c:\\users\\reneeth\\anaconda3\\envs\\r_data_analysis\\lib\\site-packages (0.53.1)\n",
      "Requirement already satisfied: llvmlite<0.37,>=0.36.0rc1 in c:\\users\\reneeth\\anaconda3\\envs\\r_data_analysis\\lib\\site-packages (from numba) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.15 in c:\\users\\reneeth\\anaconda3\\envs\\r_data_analysis\\lib\\site-packages (from numba) (1.20.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\reneeth\\anaconda3\\envs\\r_data_analysis\\lib\\site-packages (from numba) (52.0.0.post20210125)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns  #A statistical plotting library\n",
    "from sklearn.cluster import KMeans\n",
    "from kneed import KneeLocator  #A function that helps in optimization of number of clusters from an error curve\n",
    "from scipy.stats import multivariate_normal as mvn\n",
    "from numba import njit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "blond-environment",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.086590</td>\n",
       "      <td>-0.818173</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.410383</td>\n",
       "      <td>-0.388660</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.402400</td>\n",
       "      <td>-0.332094</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.356568</td>\n",
       "      <td>-0.530018</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.387783</td>\n",
       "      <td>-0.748299</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         x1        x2  Class\n",
       "0 -0.086590 -0.818173    0.0\n",
       "1  0.410383 -0.388660    0.0\n",
       "2  0.402400 -0.332094    0.0\n",
       "3  0.356568 -0.530018    0.0\n",
       "4 -0.387783 -0.748299    0.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "header_names = ['x1', 'x2' , 'Class']\n",
    "D = pd.read_csv('Data/train.csv', header = None, names = header_names)\n",
    "D.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "moral-handy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600,)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L_df = D.loc[:,['x1','x2']]\n",
    "Unlab_Data = L_df.to_numpy()\n",
    "\n",
    "lab_df = D.loc[:,'Class']\n",
    "labels = lab_df.to_numpy()\n",
    "\n",
    "#Training Dataset for Class 0\n",
    "L0 = (D['Class'] == 0.0)\n",
    "L0_df = D.loc[L0 , ['x1', 'x2'] ]  \n",
    "Class0 = L0_df.to_numpy()\n",
    "\n",
    "#Training Dataset for Class 1\n",
    "L1 = (D['Class'] == 1.0)\n",
    "L1_df = D.loc[L1 , ['x1', 'x2']]\n",
    "Class1 = L1_df.to_numpy()\n",
    "\n",
    "#Training Dataset for Class 2\n",
    "L2 = (D['Class'] == 2.0)\n",
    "L2_df = D.loc[L2, ['x1', 'x2']]\n",
    "Class2 = L2_df.to_numpy()\n",
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "protective-constant",
   "metadata": {},
   "outputs": [],
   "source": [
    "#KMeans implementation for initialization and optimization of the number of clusters\n",
    "#Number of clusters for each class eqauls the number of gaussian componenets to be fitted for that class\n",
    "def K_Clustering(Class,M):\n",
    "    #Dictionary of the arguments for scikit.KMeans\n",
    "    KMeans_args = {\n",
    "        \"init\" :\"random\",\n",
    "        \"n_init\" : 10,\n",
    "        \"max_iter\" : 300,\n",
    "        \"random_state\" : 0,\n",
    "        }\n",
    "    #Estimation of the optimum number of clusters using elbow method\n",
    "    std_error = []\n",
    "    for cluster in range(1,11):\n",
    "        kmeans = KMeans(n_clusters = cluster , **KMeans_args)\n",
    "        kmeans.fit(Class)\n",
    "        std_error.append(kmeans.inertia_)\n",
    "    if M==0:\n",
    "        #detecting the elbow point of the curve of 's_err vs K' using kneed, which gives the optimum number of clusters\n",
    "        curve = KneeLocator(range(1,11), std_error, curve=\"convex\", direction = \"decreasing\")\n",
    "        K_opt = curve.elbow\n",
    "    else:\n",
    "        #Using Manually entered value for K_opt\n",
    "        K_opt = M\n",
    "    #clustering the class in to K_opt clusters \n",
    "    kmeans =  KMeans(n_clusters = K_opt , **KMeans_args)\n",
    "    kmeans.fit(Class)\n",
    "    labels = kmeans.labels_\n",
    "    centers = kmeans.cluster_centers_\n",
    "    return K_opt,labels,centers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "prime-authentication",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        \n",
    "#initialization of the parameters using K-Clusters\n",
    "\n",
    "def Parameters_old(Class,M):\n",
    "    #Will return a mean(mu)-(K,d) array;\n",
    "    N,d = np.shape(Class)\n",
    "    K,lab,mu = K_Clustering(Class,M)\n",
    "    #gamma contains initial responsibilty values for an example w.r.t each clusters as columns\n",
    "    gamma = np.array([ [0]*K for i in range(N)])\n",
    "    for example in range(N):\n",
    "        for cluster in range(K):\n",
    "            if lab[example] == cluster:\n",
    "                gamma[example][cluster]= 1\n",
    "    return K,mu,gamma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "finnish-light",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Defining the Gaussian Mixture Model as a class\n",
    "\n",
    "class Gaussian_Mixture_Model:\n",
    "    #Class - Examples of the class to which the Gaussian Componenets need to be fitted\n",
    "    #Class - N x d matrix, where N is the number of examples and d is the number of features for each example\n",
    "    #K - Number of Gaussian Components that needs to be fitted\n",
    "    \n",
    "    def __init__(self,Class,K,MU,GAMMA,f): \n",
    "        self.Class = Class\n",
    "        self.K = K   #Attribute for Number of clusters\n",
    "        self.GAMMA = GAMMA          #Attribute for NxK array of posterior responsibity term \n",
    "        self.MU = MU                #Attribute for the mean values. An Kxd array.\n",
    "        self.SIGMA = None           #Attribute for (K,d,d) array of covariances\n",
    "        self.W = None               #Attribute for prior probabilty, an array of length K\n",
    "        #self.max_iter = max_iter   #Attribute for the number of iterations\n",
    "        self.N = len(self.Class)    #Attribute for number of examples available\n",
    "        self.d = len(self.Class[0]) #Attribute for the number of features in each example\n",
    "        self.f = f                  #Attribute that acts as switch between diagonal and full covariance matrix\n",
    "        self.mean_shift = np.reshape(self.Class, (self.N, 1, self.d) ) - np.reshape(self.MU, (1, self.K, self.d) )\n",
    "    \n",
    "    def Prior_Probability(self):\n",
    "        #A function to estimate the (K,) array of prior prob.\n",
    "        self.W = np.einsum(\"ij -> j\",self.GAMMA) / self.N  \n",
    "        \n",
    "    def Mean(self):\n",
    "      # A function to calculate mean\n",
    "      self.MU =  ((self.GAMMA).T) @ (self.Class) / np.reshape((self.W*self.N), (self.K, 1)) \n",
    "  \n",
    "    def Covariance_Matrix_Array(self):\n",
    "        # A function to calculate covariances of the features of the examples\n",
    "        \n",
    "        Nk = np.einsum(\"ij -> j\",self.GAMMA)\n",
    "        self.mean_shift = np.reshape(self.Class, (self.N, 1, self.d) ) - np.reshape(self.MU, (1, self.K, self.d) )\n",
    "        sigma = np.einsum(\"nki,nkj->kij\", np.einsum(\"nk,nki->nki\", self.GAMMA, self.mean_shift), \n",
    "                                   self.mean_shift) / np.reshape(Nk, (self.K, 1, 1))\n",
    "            \n",
    "        if self.f==1: #Case where we use full diagonal covariance matrix\n",
    "            self.SIGMA = sigma\n",
    "        \n",
    "        if self.f==0: #Case where we use a diagonal covariance matrix\n",
    "            I = np.identity(self.d,dtype=int) #An identity matrix of the size equal to number of feature\n",
    "            self.SIGMA = np.einsum(\"kij,ij -> kij\",sigma,I)\n",
    "  \n",
    "\n",
    "    def Gaussian_Prob(self):\n",
    "        #This function accounts for our assumption that the conditional distribution of an example is a Gaussian.\n",
    "        \n",
    "        self.Covariance_Matrix_Array()           #SIGMA gets updated to the full covariance matrix\n",
    "        SIGMA_inv = np.linalg.inv(self.SIGMA)     #Inverse of the covariance matrix\n",
    "        \n",
    "        norm = np.sqrt(((2*np.pi)**self.d)*np.linalg.det(self.SIGMA))  #Normalisation term of the Gaussian dist.\n",
    "        #Exponential term of the Gaussian\n",
    "        expo = np.exp(-0.5*(np.einsum(\"nkj,nkj->nk\", np.einsum(\"nki,kij->nkj\", self.mean_shift, SIGMA_inv),\n",
    "                                     self.mean_shift)))  \n",
    "        \n",
    "        #Prob_mat is an (NxK)-array that contains Gaussian Prob. of the various examples to belong to \n",
    "        #respective clusters \n",
    "        Prob_mat =  expo / norm\n",
    "        return Prob_mat\n",
    "    \n",
    "    \n",
    "    def Expectation_Step(self):\n",
    "        #In this step we update the values of the responsibilty term\n",
    "        \n",
    "        N = self.Gaussian_Prob()\n",
    "        self.W =  np.einsum(\"ij -> j\",self.GAMMA) / self.N  #Prior probability array\n",
    "        Num =  N * self.W\n",
    "        Den = np.reshape(np.sum(Num, axis=1), (self.N, 1) )\n",
    "        self.GAMMA = Num/Den\n",
    "    \n",
    "      \n",
    "    def Maximization_Step(self): \n",
    "        #In this step we updtae the various parameters\n",
    "        \n",
    "        #Updation of GAMMA\n",
    "        self.Expectation_Step()\n",
    "        \n",
    "        #Updation of W\n",
    "        self.Prior_Probability()\n",
    "        \n",
    "        #Updation of Mean MU\n",
    "        self.Mean()\n",
    "        \n",
    "        #Updation of Covariance Matrix SIGMA\n",
    "        self.Covariance_Matrix_Array()\n",
    "      \n",
    "   \n",
    "    def Log_Likelihood(self):\n",
    "      \n",
    "      llhd = np.sum(np.log(self.Gaussian_Prob() @ self.W))\n",
    "    \n",
    "      return llhd\n",
    "      \n",
    "    \n",
    "    def fit(self,max_iter,threshold):\n",
    "        \n",
    "        log_likelihoods = []  #Attribute for 1D array that contains Log_Likelihood values. Size depends on the number iterations\n",
    "                                #required to converge\n",
    "        \n",
    "        \n",
    "        for i in range(max_iter):\n",
    "            self.Expectation_Step()   #Updates Gamma\n",
    "            self.Maximization_Step()  #Updates all the other parameters\n",
    "            log_likelihoods.append(self.Log_Likelihood())\n",
    "            #An if conditional for the requirement of convergence\n",
    "            if (i!=0) & ((log_likelihoods[i] - log_likelihoods[i-1]) < threshold):\n",
    "                    break\n",
    "                    \n",
    "        print(\"Number of iterations to convegre:\" ,i)\n",
    "        \n",
    "        \n",
    "#         #Plotting log_likelihood vs iterations, comment out if not needed\n",
    "#         sns.set_style(\"darkgrid\")         #setting the plot style\n",
    "#         fig = plt.figure(figsize=(10,10))\n",
    "#         ax0 = fig.add_subplot(111) \n",
    "#         ax0.set_title('Log-Likelihood')\n",
    "#         ax0.plot(range(i+1),log_likelihoods)  \n",
    "        \n",
    "#         #Plot of the fitted Gaussians for each class\n",
    "#         x,y = np.meshgrid(np.sort(self.Class[:,0]),np.sort(self.Class[:,1])) # the meshgrid for the plot\n",
    "#         XY = np.array([x.flatten(),y.flatten()]).T \n",
    "#         sns.set_style(\"darkgrid\")         #setting the plot style\n",
    "#         fig1 = plt.figure(figsize=(10,10))\n",
    "#         ax1 = fig1.add_subplot(111)\n",
    "#         ax1.set_title('Fitted Gaussians')\n",
    "#         ax1.scatter(self.Class[:,0],self.Class[:,1],c= 'r')\n",
    "#         for mu,sigma in zip(self.MU,self.SIGMA):\n",
    "#             multi_normal = mvn(mean=mu,cov=sigma)\n",
    "#             ax1.contour(np.sort(self.Class[:,0]),np.sort(self.Class[:,1]),multi_normal.pdf(XY).reshape(len(self.Class),len(self.Class)),colors='black',alpha=0.3)\n",
    "            \n",
    "    \n",
    "    def Class_Prob(self,Y):\n",
    "            #call this for prediction of 1b\n",
    "            #A function that returns Prob. for a unlabelled vector Y to belong to a class\n",
    "            #Pred_Prob = []\n",
    "            Multi_Gauss = []\n",
    "            for mu,sigma in zip(self.MU,self.SIGMA):\n",
    "                Multi_Gauss.append(mvn(mean=mu,cov=sigma).pdf(Y)) #An array of Multi-Variate Gaussian Prob of various clusters                                                                         \n",
    "            Wt_Gauss = np.einsum(\"i,i->i\",self.W,Multi_Gauss) #An array of weighted probabilities\n",
    "            Pred_Prob =np.sum(Wt_Gauss)  \n",
    "            return Pred_Prob\n",
    "        \n",
    "    def Class_Prob_set_features(self,L):\n",
    "        #call this for prediction of 2b\n",
    "        #A function that returns Prob. for a set of feature vectors. For example, the set of all local feature \n",
    "        #vectors of an image. L will be an 2D array-(nxd).\n",
    "        n = L.shape[0]   #Number of local feature row vectors\n",
    "        d = L.shape[1]   #dimension  of feature space\n",
    "        Prob_nfeature_list = []\n",
    "        for n_feature in range(n):\n",
    "            Prob_nfeature_list.append(self.Class_Prob(L[n_feature,:d]))\n",
    "        \n",
    "        Prob_nfeature_arr = np.array(Prob_nfeature_list)\n",
    "        Pred_Prob = np.product(Prob_nfeature_arr)\n",
    "        return Pred_Prob\n",
    "            \n",
    "        \n",
    "        \n",
    "            \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "equipped-rescue",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations to convegre: 26\n",
      "Wall time: 21.2 ms\n"
     ]
    }
   ],
   "source": [
    "#Fitting gaussian mixtures for Class0 \n",
    "K,MU,GAMMA = Parameters_old(Class0,0)  #0 as the second argument chooses by default K_opt estimated using elbow method. \n",
    "                                       #If not pass the number of clusters needed\n",
    "\n",
    "gmm0 = Gaussian_Mixture_Model(Class0,K,MU,GAMMA,1) #0 as the last argument -> diagonal covariance matrix. 1-> full covariance matix.   \n",
    "#gmm0.fit(max_iter=100,threshold = 1e-10)\n",
    "%time _ = gmm0.fit(max_iter=100,threshold = 1e-10)\n",
    "Y = [0.410383 ,-0.388660]\n",
    "#gmm0.Class_Prob(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "general-adult",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations to convegre: 1\n",
      "Wall time: 4.98 ms\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "caroline-pulse",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations to convegre: 23\n"
     ]
    }
   ],
   "source": [
    "#Fitting gaussian mixtures for Class1 \n",
    "K,MU,GAMMA = Parameters_old(Class1,0)\n",
    "gmm1 = Gaussian_Mixture_Model(Class1,K,MU,GAMMA,1)\n",
    "gmm1.fit(max_iter=100,threshold = 1e-10)\n",
    "#gmm1.Class_Prob(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fifty-behalf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations to convegre: 67\n"
     ]
    }
   ],
   "source": [
    "#Fitting gaussian mixtures for Class2 \n",
    "K,MU,GAMMA = Parameters_old(Class2,0)\n",
    "gmm2 = Gaussian_Mixture_Model(Class2,K,MU,GAMMA,1)\n",
    "gmm2.fit(max_iter=100,threshold = 1e-10)\n",
    "\n",
    "#gmm2.Class_Prob(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "difficult-pointer",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We have fitted 3 gaussians to each class and now we would like to make prediction for unlabelled points\n",
    "def Class_Prediction(Y):\n",
    "    #gmm0,gmm1,gmm2 are the instances of class 0, class 1 and class 2 respectively\n",
    "    n = len(Y) #number of unlabelled examples\n",
    "    prediction = []\n",
    "    for example in range(n):\n",
    "        Prob=[]\n",
    "        Prob = [gmm0.Class_Prob(Y[example,:]), gmm1.Class_Prob(Y[example,:]),gmm2.Class_Prob(Y[example,:]) ]\n",
    "        prediction.append(np.argmax(Prob))\n",
    "    #print(\"Labels for the given dataset:\", prediction)\n",
    "    return prediction\n",
    "# Pred = Class_Prediction(Unlab_Data)\n",
    "# lst3 = [value for value in Pred if value in labels]\n",
    "# lst3\n",
    "# arr = os.listdir('.\\Data\\Dataset_2B\\coast\\train')\n",
    "# print(arr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "turned-silence",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "quarterly-communication",
   "metadata": {},
   "source": [
    "# Real World Dataset_2b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "capital-recovery",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import QuantileTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forbidden-wellington",
   "metadata": {},
   "source": [
    "## Loading the dataset as an array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "wrapped-maria",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file_as_arr(Dir):\n",
    "    #Returns a 3D array DAT\n",
    "    dat = [] #the list that needs to be converted to 3D array DAT\n",
    "    for image in os.listdir(Dir):\n",
    "        f = open(Dir +'/'+image)\n",
    "        single_image_dat = []\n",
    "        for line in f:\n",
    "            single_image_dat.append([float(x) for x in line.strip().split(' ')])\n",
    "        dat.append(single_image_dat)\n",
    "    DAT = np.array(dat)\n",
    "    return DAT,len(DAT)     #len(DAT) is needed for prior prob. calculation as the number of \n",
    "                            #examples are different for each class\n",
    "\n",
    "\n",
    "#Class coast training images        \n",
    "Dir_ctr = 'Data/Dataset_2B/coast/train'             \n",
    "coast,n_c = load_file_as_arr(Dir_ctr) \n",
    "\n",
    "\n",
    "\n",
    "#Class forest training images\n",
    "Dir_ftr = 'Data/Dataset_2B/forest/train' \n",
    "forest,n_f = load_file_as_arr(Dir_ftr)\n",
    "\n",
    "\n",
    "#Class mountain training images\n",
    "Dir_mtr = 'Data/Dataset_2B/mountain/train'  \n",
    "mountain,n_m = load_file_as_arr(Dir_mtr)\n",
    "\n",
    "\n",
    "#Class opencountry training images\n",
    "Dir_otr = 'Data/Dataset_2B/opencountry/train'  \n",
    "opencountry, n_o = load_file_as_arr(Dir_otr)\n",
    "\n",
    "#Class street training images\n",
    "Dir_str = 'Data/Dataset_2B/street/train'  \n",
    "street, n_s = load_file_as_arr(Dir_str)\n",
    "\n",
    "Dir_coast_dev = 'Data/Dataset_2B/coast/dev' \n",
    "coast_d,n_cd = \n",
    "\n",
    "#Prior prob. for respective classes as an 1x5 array\n",
    "initial_weight = np.array([n_c, n_f, n_m, n_o, n_s])\n",
    "prior_prob = initial_weight/np.sum(initial_weight)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "popular-liquid",
   "metadata": {},
   "source": [
    "## A Class for Scaling of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "indian-bowling",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining a class for proper scaling of data\n",
    "#Based on a stackoverflow answer\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "\n",
    "class Preprocess_Data(TransformerMixin):  #Dir is the location of all the data files from images of the class\n",
    "    #An advantage of using this class is each feature will be scaled differently using QuantileTransformer.\n",
    "    \n",
    "    \n",
    "    def __init__(self,n_quant, **kwargs):\n",
    "        self._scaler = QuantileTransformer(n_quantiles = n_quant,copy=True, **kwargs)\n",
    "        self._orig_shape = None\n",
    "\n",
    "    def fit(self, X, **kwargs):\n",
    "        X = np.array(X)\n",
    "        # Save the original shape to reshape the flattened X later\n",
    "        # back to its original shape\n",
    "        if len(X.shape) > 1:\n",
    "            self._orig_shape = X.shape[1:]\n",
    "        X = self._flatten(X)\n",
    "        self._scaler.fit(X, **kwargs)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, **kwargs):\n",
    "        X = np.array(X)\n",
    "        X = self._flatten(X)\n",
    "        X = self._scaler.transform(X, **kwargs)\n",
    "        X = self._reshape(X)\n",
    "        return X\n",
    "\n",
    "    def _flatten(self, X):\n",
    "        # Reshape X to <= 2 dimensions\n",
    "        if len(X.shape) > 2:\n",
    "            X = X.reshape(X.shape[0],-1)  #Converts 3d array in to two dimensions\n",
    "        return X\n",
    "    \n",
    "    \n",
    "    def _reshape(self, X):\n",
    "        # Reshape X back to it's original shape\n",
    "        if len(X.shape) >= 2:\n",
    "            X = X.reshape(-1, *self._orig_shape)\n",
    "        return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optimum-kinase",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "rolled-fleet",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Preprocess_Data at 0x21b3f28e910>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Coast images preprocessing\n",
    "\n",
    "coast_scaler = Preprocess_Data(len(coast))\n",
    "coast_scaler.fit(coast)  #flattens and scales\n",
    "coast_processed = coast_scaler.transform(coast) #converts the flattened array back to original shape\n",
    "#coast_processed[0,:,:].T\n",
    "\n",
    "#forest images preprocesing\n",
    "forest_scaler = Preprocess_Data(len(forest))\n",
    "forest_scaler.fit(forest)\n",
    "forest_processed = forest_scaler.transform(forest)\n",
    "\n",
    "#mountain images preprocesing\n",
    "mountain_scaler = Preprocess_Data(len(mountain))\n",
    "mountain_scaler.fit(mountain)\n",
    "mountain_processed = mountain_scaler.transform(mountain)\n",
    "\n",
    "\n",
    "\n",
    "#opencountry images preprocesing\n",
    "opencountry_scaler = Preprocess_Data(len(opencountry))\n",
    "opencountry_scaler.fit(opencountry)\n",
    "opencountry_processed = opencountry_scaler.transform(opencountry)\n",
    "\n",
    "\n",
    "#street images preprocessing\n",
    "street_scaler = Preprocess_Data(len(street))\n",
    "street_scaler.fit(street)\n",
    "street_processed = street_scaler.transform(street)\n",
    "\n",
    "coast_scaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extraordinary-identification",
   "metadata": {},
   "source": [
    "#### Principal Componenet Analysis from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "mighty-desire",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA for each image to determine the optimum number of features\n",
    "\n",
    "class Transform_Features:\n",
    "    #Will tranform the 23 dimensional feature vector to a lower dimension with more than 95% variance explained\n",
    "    \n",
    "    def __init__(self,Class):\n",
    "        \n",
    "        self.Class = Class           #Class should be an Nxnxd array.\n",
    "        self.N = Class.shape[0]      #Attribute for the number of training images of the given class.\n",
    "        self.n = Class.shape[1]      #Attribute for the total number of feature row vectors of the image\n",
    "                                     #or the number of boxes the image is divided in to.        \n",
    "        self.d = Class.shape[2]      #Attribute for dimension of the feature row vector for each thus divided box.\n",
    "        self.var_explained = None    #Attribute for an percentage variance explained by different dimensions. \n",
    "                                     #Dim-d explains 100 percent.\n",
    "        self.cum_var_explained = None\n",
    "        \n",
    "        self.dim_opt = None\n",
    "        self.eig_val_arr = None\n",
    "        self.eig_vec_arr = None\n",
    "        #self.T_Class_image = None \n",
    "        \n",
    "            \n",
    "        \n",
    "    def Eigen_Decomposition(self,X):\n",
    "        #Returns Eigen Values and Eigen Vectors.\n",
    "        \n",
    "        sigma = np.cov(X.T)        #X should be an nxd array, then sigma will be an dxd covriance matrix.\n",
    "        \n",
    "        #Eigendecomposition exists for all positive, real valued symmetric matrices and sigma is one such matrix.\n",
    "        self.eig_val_arr,self.eig_vec_arr = np.linalg.eig(sigma)\n",
    "        #return eig_val_arr,eig_vec_arr\n",
    "    \n",
    "    \n",
    "    def Reduce_Dimension_Image(self,X):\n",
    "        #Determines the optimum number of dimensions of the feature space for an input nxd array(i.e) a single image.\n",
    "        self.Eigen_Decomposition(X) #Will update eig_val_arr,eig_vec_arr\n",
    "        \n",
    "        norm = 100.0/np.einsum(\"i-> \",self.eig_val_arr) \n",
    "        self.var_explained = (self.eig_val_arr*norm)\n",
    "        \n",
    "        self.cum_var_explained = np.cumsum(self.var_explained)\n",
    "        \n",
    "        #Optimum number of dimensions using elbow method\n",
    "        curve = KneeLocator(range(1,(len(self.cum_var_explained))+1), self.cum_var_explained, curve=\"concave\", direction = \"increasing\")\n",
    "        dim_opt_img = curve.elbow\n",
    "        \n",
    "        return dim_opt_img\n",
    "    \n",
    "    \n",
    "    def Reduced_Dimension(self):\n",
    "        #An itheration of Reduced_Dimension_Image over all images of the class and choosing the final reduced dimension.\n",
    "        dim_opt_list = []\n",
    "        \n",
    "        for image in range(self.N):\n",
    "            dim_opt_list.append(self.Reduce_Dimension_Image(self.Class[image,:,:]))\n",
    "        \n",
    "        self.dim_opt = max(dim_opt_list)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def Transform_Class(self,dim):\n",
    "        #A function that produces the transformation matrix that reduces the dimension of the feature vectors to dim.\n",
    "        \n",
    "        T_img_list = [] \n",
    "        \n",
    "        for image in range(self.N):\n",
    "            self.Eigen_Decomposition(self.Class[image,:,:])              #Will update eig_val_arr and eig_vec_arr.\n",
    "            T_mat_image = (self.eig_vec_arr.T[:][:dim]).T  #Transformation matrix for the image\n",
    "#             T_img_list.append(np.einsum(\"ik,kj -> ij\",self.Class[image,:,:],T_mat_image))  #Transform the image in to lower dimension.\n",
    "            T_img_list.append(self.Class[image,:,:].dot(T_mat_image))  #using np.dot is faster.\n",
    "    \n",
    "        Transformed_Class = np.array(T_img_list) #Nxnxdim_opt array of all transformed images\n",
    "        return Transformed_Class\n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "unsigned-keyboard",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Estimation of optimum number of dimensions for feature vectors\n",
    "\n",
    "\n",
    "coast_inst = Transform_Features(coast_processed)             #Instance for class coast,pass in the processed data.\n",
    "coast_inst.Reduced_Dimension()                               #Updates the attribute for the opt number of clusters for forest class.\n",
    "\n",
    "\n",
    "forest_inst = Transform_Features(forest_processed)           #instance for class forest,pass in the preprocessed data.\n",
    "forest_inst.Reduced_Dimension()                              #Updates the attribute for opt number of clusters for forest class.\n",
    "\n",
    "\n",
    "mountain_inst = Transform_Features(mountain_processed)       #instance for class mountain,pass in the preprocessed data.\n",
    "mountain_inst.Reduced_Dimension()                            #Updates the attribute for opt number of clusters for mountain class.\n",
    "\n",
    "\n",
    "opencountry_inst = Transform_Features(opencountry_processed) #instance for class opencountry,pass in the preprocessed data.\n",
    "opencountry_inst.Reduced_Dimension()                         #Updates the attribute for opt number of clusters for opencountry class.\n",
    "\n",
    "\n",
    "street_inst = Transform_Features(street_processed)           #instance for class street,pass in the preprocessed data.\n",
    "street_inst.Reduced_Dimension()                              #Updates the attribute for opt number of clusters for street class.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#The final chosen dimension of the transformed feature vectors\n",
    "\n",
    "red_dim= max(coast_inst.dim_opt, forest_inst.dim_opt, mountain_inst.dim_opt, opencountry_inst.dim_opt,\n",
    "             street_inst.dim_opt)  \n",
    "red_dim = 23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "identified-humanity",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preparation of training data for each class\n",
    "\n",
    "#Class-coast:\n",
    "coast_pca = coast_inst.Transform_Class(red_dim)                          #coast_pca is the images in the lower feature dimension an 3D array\n",
    "coast_tr_data = coast_pca.reshape((coast_inst.N*coast_inst.n),red_dim)   #2D array and is the training data to which gaussians will be fitted for class coast\n",
    "\n",
    "\n",
    "#Class-forest:\n",
    "forest_pca = forest_inst.Transform_Class(red_dim)\n",
    "forest_tr_data = forest_pca.reshape((forest_inst.N*forest_inst.n),red_dim)\n",
    "\n",
    "\n",
    "#Class-mountain:\n",
    "mountain_pca = mountain_inst.Transform_Class(red_dim)\n",
    "mountain_tr_data = mountain_pca.reshape((mountain_inst.N*mountain_inst.n),red_dim)\n",
    "\n",
    "\n",
    "#Class-opencountry:\n",
    "opencountry_pca = opencountry_inst.Transform_Class(red_dim)\n",
    "opencountry_tr_data = opencountry_pca.reshape((opencountry_inst.N*opencountry_inst.n),red_dim)\n",
    "\n",
    "\n",
    "#Class-street:\n",
    "street_pca = street_inst.Transform_Class(red_dim)\n",
    "street_tr_data = street_pca.reshape((street_inst.N*street_inst.n),red_dim)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "advised-foundation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations to convegre: 140\n"
     ]
    }
   ],
   "source": [
    "#Fitting GMM for class coast\n",
    "K,MU,GAMMA = Parameters_old(coast_tr_data,0)  #0 as the second argument chooses by default K_opt estimated using elbow method. \n",
    "                                             #If not pass the number of clusters needed\n",
    "\n",
    "gmm_coast = Gaussian_Mixture_Model(coast_tr_data,K,MU,GAMMA,1) #0 as the last argument -> diagonal covariance matrix. 1-> full covariance matix.   \n",
    "gmm_coast.fit(max_iter=1000,threshold = 1e-10)\n",
    "#gmm_coast.Class_Prob_set_features(coast_pca[0,:,:])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "referenced-nancy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations to convegre: 136\n"
     ]
    }
   ],
   "source": [
    "#Fitting GMM for class forest\n",
    "K,MU,GAMMA = Parameters_old(forest_tr_data,0)  #0 as the second argument chooses by default K_opt estimated using elbow method. \n",
    "                                             #If not pass the number of clusters needed\n",
    "\n",
    "gmm_forest = Gaussian_Mixture_Model(forest_tr_data,K,MU,GAMMA,1) #0 as the last argument -> diagonal covariance matrix. 1-> full covariance matix.   \n",
    "gmm_forest.fit(max_iter=1000,threshold = 1e-10)\n",
    "#gmm_forest.Class_Prob_set_features(coast_pca[0,:,:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "treated-variation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations to convegre: 119\n"
     ]
    }
   ],
   "source": [
    "#Fitting GMM for class mountain:\n",
    "K,MU,GAMMA = Parameters_old(mountain_tr_data,0)  #0 as the second argument chooses by default K_opt estimated using elbow method. \n",
    "                                             #If not pass the number of clusters needed\n",
    "\n",
    "gmm_mountain = Gaussian_Mixture_Model(mountain_tr_data,K,MU,GAMMA,1) #0 as the last argument -> diagonal covariance matrix. 1-> full covariance matix.   \n",
    "gmm_mountain.fit(max_iter=1000,threshold = 1e-10)\n",
    "#gmm_mountain.Class_Prob_set_features(coast_pca[0,:,:]) #was meant for testing, don't execute in the final run.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "heavy-craps",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations to convegre: 254\n"
     ]
    }
   ],
   "source": [
    "#Fitting GMM for class opencountry:\n",
    "K,MU,GAMMA = Parameters_old(opencountry_tr_data,0)  #0 as the second argument chooses by default K_opt estimated using elbow method. \n",
    "                                             #If not pass the number of clusters needed\n",
    "\n",
    "gmm_opencountry = Gaussian_Mixture_Model(opencountry_tr_data,K,MU,GAMMA,1) #0 as the last argument -> diagonal covariance matrix. 1-> full covariance matix.   \n",
    "gmm_opencountry.fit(max_iter=1000,threshold = 1e-10)\n",
    "#gmm_opencountry.Class_Prob_set_features(coast_pca[0,:,:])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "homeless-specialist",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations to convegre: 78\n"
     ]
    }
   ],
   "source": [
    "#Fitting GMM for class street:\n",
    "K,MU,GAMMA = Parameters_old(street_tr_data,0)  #0 as the second argument chooses by default K_opt estimated using elbow method. \n",
    "                                             #If not pass the number of clusters needed\n",
    "\n",
    "gmm_street = Gaussian_Mixture_Model(street_tr_data,K,MU,GAMMA,1) #0 as the last argument -> diagonal covariance matrix. 1-> full covariance matix.   \n",
    "gmm_street.fit(max_iter=1000,threshold = 1e-10)\n",
    "#gmm_street.Class_Prob_set_features(coast_pca[0,:,:])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qualified-state",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "chinese-health",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We have fitted 3 gaussians to each class and now we would like to make prediction for unlabelled points\n",
    "def Class_Prediction_set_features(Y,W):\n",
    "    #Y is a 3D array and W is the prior probability estimated in the beginning\n",
    "    \n",
    "    n = len(Y)            #number of unlabelled images\n",
    "    prediction = []\n",
    "    for image in range(n):\n",
    "        Prob=[]\n",
    "        Prob =np.array([gmm_coast.Class_Prob_set_features(Y[image,:,:]),\n",
    "                        gmm_forest.Class_Prob_set_features(Y[image,:,:]),\n",
    "                        gmm_mountain.Class_Prob_set_features(Y[image,:,:]),\n",
    "                        gmm_opencountry.Class_Prob_set_features(Y[image,:,:]),\n",
    "                        gmm_street.Class_Prob_set_features(Y[image,:,:])])\n",
    "        \n",
    "        #Prob = Prob.dot(W)  #Uncomment If we want to multiply prior prob.\n",
    "        \n",
    "        prediction.append(np.argmax(Prob))\n",
    "    #print(\"Labels for the given dataset:\", prediction)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "acknowledged-advertiser",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#for prediction pass in data that is processed and feature vectors are reduced to a (1xred_dim)- dimension. \n",
    "Class_Prediction_set_features(coast_pca[:,:,:],prior_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "binary-signature",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
